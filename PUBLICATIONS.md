# List of publications using Lingvo.



## Translation

<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu2016google">1</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Wu, M.&nbsp;Schuster, Z.&nbsp;Chen, Q.&nbsp;V. Le, M.&nbsp;Norouzi, W.&nbsp;Macherey, M.&nbsp;Krikun,
  Y.&nbsp;Cao, Q.&nbsp;Gao, K.&nbsp;Macherey, J.&nbsp;Klingner, A.&nbsp;Shah, M.&nbsp;Johnson, X.&nbsp;Liu,
  L.&nbsp;Kaiser, S.&nbsp;Gouws, Y.&nbsp;Kato, T.&nbsp;Kudo, H.&nbsp;Kazawa, K.&nbsp;Stevens, G.&nbsp;Kurian,
  N.&nbsp;Patil, W.&nbsp;Wang, C.&nbsp;Young, J.&nbsp;Smith, J.&nbsp;Riesa, A.&nbsp;Rudnick, O.&nbsp;Vinyals,
  G.&nbsp;Corrado, M.&nbsp;Hughes, and J.&nbsp;Dean, &ldquo;Google's neural machine translation
  system: Bridging the gap between human and machine translation,&rdquo; tech. rep.,
  2016.
[&nbsp;<a href="https://arxiv.org/abs/1609.08144">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="johnson-etal-2017-googles">2</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Johnson, M.&nbsp;Schuster, Q.&nbsp;V. Le, M.&nbsp;Krikun, Y.&nbsp;Wu, Z.&nbsp;Chen, N.&nbsp;Thorat,
  F.&nbsp;Vi&eacute;gas, M.&nbsp;Wattenberg, G.&nbsp;Corrado, M.&nbsp;Hughes, and J.&nbsp;Dean,
  &ldquo;Google's multilingual neural machine translation system: Enabling
  zero-shot translation,&rdquo; <em>Transactions of the Association for
  Computational Linguistics</em>, vol.&nbsp;5, pp.&nbsp;339--351, 2017.
[&nbsp;<a href="http://dx.doi.org/10.1162/tacl_a_00065">DOI</a>&nbsp;| 
<a href="https://www.aclweb.org/anthology/Q17-1024">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="eriguchi2018zero">3</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Eriguchi, M.&nbsp;Johnson, O.&nbsp;Firat, H.&nbsp;Kazawa, and W.&nbsp;Macherey, &ldquo;Zero-shot
  cross-lingual classification using multilingual neural machine translation,&rdquo;
  <em>arXiv preprint arXiv:1809.04686</em>, 2018.
[&nbsp;<a href="https://arxiv.org/pdf/1809.04686.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bapna2018training">4</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Bapna, M.&nbsp;X. Chen, O.&nbsp;Firat, Y.&nbsp;Cao, and Y.&nbsp;Wu, &ldquo;Training deeper neural
  machine translation models with transparent attention,&rdquo; in <em>Proc.
  Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>,
  2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.07561">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="cherry2018revisiting">5</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Cherry, G.&nbsp;Foster, A.&nbsp;Bapna, O.&nbsp;Firat, and W.&nbsp;Macherey, &ldquo;Revisiting
  character-based neural machine translation with capacity and compression,&rdquo;
  in <em>Proc. Conference on Empirical Methods in Natural Language Processing
  (EMNLP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.09943">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chen2018best">6</a>]
</td>
<td class="bibtexitem">
M.&nbsp;X. Chen, O.&nbsp;Firat, A.&nbsp;Bapna, M.&nbsp;Johnson, W.&nbsp;Macherey, G.&nbsp;Foster, L.&nbsp;Jones,
  M.&nbsp;Schuster, N.&nbsp;Shazeer, N.&nbsp;Parmar, A.&nbsp;Vaswani, J.&nbsp;Uszkoreit, L.&nbsp;Kaiser,
  Z.&nbsp;Chen, Y.&nbsp;Wu, and M.&nbsp;Hughes, &ldquo;The Best of Both Worlds: Combining Recent
  Advances in Neural Machine Translation,&rdquo; in <em>Proc. Annual Meeting of
  the Association for Computational Linguistics (ACL)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1804.09849">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kuczmarski2018gender">7</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Kuczmarski and M.&nbsp;Johnson, &ldquo;Gender-aware natural language translation,&rdquo;
  2018.
[&nbsp;<a href="https://www.tdcommons.org/dpubs_series/1577/">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="aharoni2019massively">8</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Aharoni, M.&nbsp;Johnson, and O.&nbsp;Firat, &ldquo;Massively multilingual neural machine
  translation,&rdquo; 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1903.00089.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="luo2019neural">9</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Luo, Y.&nbsp;Cao, and R.&nbsp;Barzilay, &ldquo;Neural decipherment via minimum-cost flow:
  From ugaritic to linear b,&rdquo; 2019.
[&nbsp;<a href="https://www.aclweb.org/anthology/P19-1303/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="milk">10</a>]
</td>
<td class="bibtexitem">
N.&nbsp;Arivazhagan, C.&nbsp;Cherry, W.&nbsp;Macherey, C.-C. Chiu, S.&nbsp;Yavuz, R.&nbsp;Pang, W.&nbsp;Li,
  and C.&nbsp;Raffel, &ldquo;Monotonic infinite lookback attention for simultaneous
  machine translation,&rdquo; in <em>Proc. Annual Meeting of the Association for
  Computational Linguistics (ACL)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1906.05218">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="freitag2019text">11</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Freitag, I.&nbsp;Caswell, and S.&nbsp;Roy, &ldquo;Ape at scale and its implications on mt
  evaluation biases,&rdquo; 2019.
[&nbsp;<a href="https://arxiv.org/abs/1904.04790">pdf</a>&nbsp;| 
<a href="https://www.aclweb.org/anthology/W19-5204">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="arivazhagan2019">12</a>]
</td>
<td class="bibtexitem">
N.&nbsp;Arivazhagan, A.&nbsp;Bapna, O.&nbsp;Firat, D.&nbsp;Lepikhin, M.&nbsp;Johnson, M.&nbsp;Krikun, M.&nbsp;X.
  Chen, Y.&nbsp;Cao, G.&nbsp;Foster, C.&nbsp;Cherry, W.&nbsp;Macherey, Z.&nbsp;Chen, and Y.&nbsp;Wu,
  &ldquo;Massively multilingual neural machine translation in the wild: Findings and
  challenges,&rdquo; 2019.
[&nbsp;<a href="http://arxiv.org/abs/1907.05019">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1907.05019">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="huang2019gpipe">13</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Huang, Y.&nbsp;Cheng, A.&nbsp;Bapna, O.&nbsp;Firat, M.&nbsp;X. Chen, D.&nbsp;Chen, H.&nbsp;Lee, J.&nbsp;Ngiam,
  Q.&nbsp;V. Le, Y.&nbsp;Wu, and Z.&nbsp;Chen, &ldquo;Gpipe: Efficient training of giant neural
  networks using pipeline parallelism,&rdquo; in <em>Advances in Neural Information
  Processing Systems</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.06965">http</a>&nbsp;]

</td>
</tr>
</table>

## Speech recognition

<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chiu2018state">1</a>]
</td>
<td class="bibtexitem">
C.-C.Chiu, T.&nbsp;N. Sainath, Y.&nbsp;Wu, R.&nbsp;Prabhavalkar, P.&nbsp;Nguyen, Z.&nbsp;Chen,
  A.&nbsp;Kannan, R.&nbsp;J. Weiss, K.&nbsp;Rao, K.&nbsp;Gonina, N.&nbsp;Jaitly, B.&nbsp;Li, J.&nbsp;Chorowski,
  and M.&nbsp;Bacchiani, &ldquo;State-of-the-art speech recognition with
  sequence-to-sequence models,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01769">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="toshniwal2018multilingual">2</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Toshniwal, T.&nbsp;N. Sainath, R.&nbsp;J. Weiss, B.&nbsp;Li, P.&nbsp;Moreno, E.&nbsp;Weinstein, and
  K.&nbsp;Rao, &ldquo;Multilingual speech recognition with a single end-to-end model,&rdquo;
  in <em>Proc. IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1711.01694">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2018multidialect">3</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, T.&nbsp;N. Sainath, K.&nbsp;Sim, M.&nbsp;Bacchiani, E.&nbsp;Weinstein, P.&nbsp;Nguyen, Z.&nbsp;Chen,
  Y.&nbsp;Wu, and K.&nbsp;Rao, &ldquo;Multi-Dialect Speech Recognition With a Single
  Sequence-to-Sequence Model,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01541">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sainath2018no">4</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, P.&nbsp;Prabhavalkar, S.&nbsp;Kumar, S.&nbsp;Lee, A.&nbsp;Kannan, D.&nbsp;Rybach,
  V.&nbsp;Schogol, P.&nbsp;Nguyen, B.&nbsp;Li, Y.&nbsp;Wu, Z.&nbsp;Chen, and C.&nbsp;C. Chiu, &ldquo;No Need for
  a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End
  Models,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01864">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lawson2018learning">5</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Lawson, C.&nbsp;C. Chiu, G.&nbsp;Tucker, C.&nbsp;Raffel, K.&nbsp;Swersky, and N.&nbsp;Jaitly,
  &ldquo;Learning hard alignments with variational inference,&rdquo; in <em>Proc.
  IEEE International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1705.05524">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kannan2018analysis">6</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Kannan, Y.&nbsp;Wu, P.&nbsp;Nguyen, T.&nbsp;N. Sainath, Z.&nbsp;Chen, and R.&nbsp;Prabhavalkar, &ldquo;An
  analysis of incorporating an external language model into a
  sequence-to-sequence model,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01996">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="prabhavalkar2018minimum">7</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Prabhavalkar, T.&nbsp;N. Sainath, Y.&nbsp;Wu, P.&nbsp;Nguyen, Z.&nbsp;Chen, C.&nbsp;C. Chiu, and
  A.&nbsp;Kannan, &ldquo;Minimum Word Error Rate Training for Attention-based
  Sequence-to-sequence Models,&rdquo; in <em>Proc. IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01818">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sainath2018improving">8</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, C.&nbsp;C. Chiu, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, Y.&nbsp;Wu, P.&nbsp;Nguyen, and
  Z.&nbsp;C. Z, &ldquo;Improving the Performance of Online Neural Transducer Models,&rdquo;
  in <em>Proc. IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.01807">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chiu2018monotonic">9</a>]
</td>
<td class="bibtexitem">
C.&nbsp;C. Chiu and C.&nbsp;Raffel, &ldquo;Monotonic Chunkwise Attention,&rdquo; in <em>Proc.
  International Conference on Learning Representations (ICLR)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1712.05382">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="williams2018contextual">10</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Williams, A.&nbsp;Kannan, P.&nbsp;Aleksic, D.&nbsp;Rybach, and T.&nbsp;N.&nbsp;S. TN, &ldquo;Contextual
  Speech Recognition in End-to-End Neural Network Systems using Beam Search,&rdquo;
  in <em>Proc. Interspeech</em>, 2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/2416.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chui2018speech">11</a>]
</td>
<td class="bibtexitem">
C.&nbsp;C. Chiu, A.&nbsp;Tripathi, K.&nbsp;Chou, C.&nbsp;Co, N.&nbsp;Jaitly, D.&nbsp;Jaunzeikare, A.&nbsp;Kannan,
  P.&nbsp;Nguyen, H.&nbsp;Sak, A.&nbsp;Sankar, J.&nbsp;Tansuwan, N.&nbsp;Wan, Y.&nbsp;Wu, and X.&nbsp;Zhang,
  &ldquo;Speech recognition for medical conversations,&rdquo; in <em>Proc.
  Interspeech</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1711.07274">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pang2018compression">12</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Pang, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, S.&nbsp;Gupta, Y.&nbsp;Wu, S.&nbsp;Zhang, and C.&nbsp;C.
  Chiu, &ldquo;Compression of End-to-End Models,&rdquo; in <em>Proc. Interspeech</em>,
  2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1025.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="toshniwal2018comparison">13</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Toshniwal, A.&nbsp;Kannan, C.&nbsp;C. Chiu, Y.&nbsp;Wu, T.&nbsp;N. Sainath, and K.&nbsp;Livescu, &ldquo;A
  comparison of techniques for language model integration in encoder-decoder
  speech recognition,&rdquo; in <em>Proc. IEEE Spoken Language Technology
  Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1807.10857">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pundak2018deep">14</a>]
</td>
<td class="bibtexitem">
G.&nbsp;Pundak, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, and D.&nbsp;Zhao, &ldquo;Deep
  context: End-to-end contextual speech recognition,&rdquo; in <em>Proc. IEEE
  Spoken Language Technology Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1808.02480">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2019bytes">15</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, Y.&nbsp;Zhang, T.&nbsp;N. Sainath, Y.&nbsp;Wu, and W.&nbsp;Chan, &ldquo;Bytes are all you need:
  End-to-end multilingual speech recognition and synthesis with bytes,&rdquo; in
  <em>Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.09021">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="guo2019spelling">16</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Guo, T.&nbsp;N. Sainath, and R.&nbsp;J. Weiss, &ldquo;A spelling correction model for
  end-to-end speech recognition,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1902.07178">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="alon2019contextual">17</a>]
</td>
<td class="bibtexitem">
U.&nbsp;Alon, G.&nbsp;Pundak, and T.&nbsp;N. Sainath, &ldquo;Contextual speech recognition with
  difficult negative training examples,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1810.12170">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="qin2019imperceptible">18</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Qin, N.&nbsp;Carlini, I.&nbsp;Goodfellow, G.&nbsp;Cottrell, and C.&nbsp;Raffel, &ldquo;Imperceptible,
  robust, and targeted adversarial examples for automatic speech recognition,&rdquo;
  in <em>Proc. International Conference on Machine Learning (ICML)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1903.10346">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="park2019specaugment">19</a>]
</td>
<td class="bibtexitem">
D.&nbsp;S. Park, W.&nbsp;Chan, Y.&nbsp;Zhang, C.&nbsp;Chiu, B.&nbsp;Zoph, E.&nbsp;D. Cubuk, and Q.&nbsp;V. Le,
  &ldquo;SpecAugment: A Simple Data Augmentation Method for Automatic Speech
  Recognition,&rdquo; in <em>arXiv</em>, 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1904.08779.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li2019semisupervised">20</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Li, T.&nbsp;N. Sainath, R.&nbsp;Pang, and Z.&nbsp;Wu, &ldquo;Semi-supervised training for
  end-to-end models via weak distillation,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682172">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chang2019joint">21</a>]
</td>
<td class="bibtexitem">
S.-Y. Chang, R.&nbsp;Prabhavalkar, Y.&nbsp;He, T.&nbsp;N. Sainath, and G.&nbsp;Simko, &ldquo;Joint
  endpointing and decoding with end-to-end models,&rdquo; in <em>Proc. IEEE
  International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8683109">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="heymann2019improving">22</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Heymann, K.&nbsp;C. Sim, and B.&nbsp;Li, &ldquo;Improving ctc using stimulated learning for
  sequence modeling,&rdquo; in <em>Proc. IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682700">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bruguier2019phoebe">23</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Bruguier, R.&nbsp;Prabhavalkar, G.&nbsp;Pundak, and T.&nbsp;N. Sainath, &ldquo;Phoebe:
  Pronunciation-aware contextualization for end-to-end speech recognition,&rdquo; in
  <em>Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://ieeexplore.ieee.org/document/8682441">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="he2019streaming">24</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;He, T.&nbsp;N. Sainath, R.&nbsp;Prabhavalkar, I.&nbsp;McGraw, R.&nbsp;Alvarez, D.&nbsp;Zhao,
  D.&nbsp;Rybach, A.&nbsp;Kannan, Y.&nbsp;Wu, R.&nbsp;Pang, Q.&nbsp;Liang, D.&nbsp;Bhatia, Y.&nbsp;Shangguan,
  B.&nbsp;Li, G.&nbsp;Pundak, K.&nbsp;C. Sim, T.&nbsp;Bagby, S.-Y. Chang, K.&nbsp;Rao, and
  A.&nbsp;Gruenstein, &ldquo;Streaming end-to-end speech recognition for mobile
  devices,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.06621">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="irie2019unit">25</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Irie, R.&nbsp;Prabhavalkar, A.&nbsp;Kannan, A.&nbsp;Bruguier, D.&nbsp;Rybach, and P.&nbsp;Nguyen,
  &ldquo;Model unit exploration for sequence-to-sequence speech recognition,&rdquo; <em>
  arXiv e-prints</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1902.01955">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="peyser2019numerics">26</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Peyser, H.&nbsp;Zhang, T.&nbsp;N. Sainath, and Z.&nbsp;Wu, &ldquo;Improving Performance of
  End-to-End ASR on Numeric Sequences,&rdquo; in <em>Proc. Interspeech</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1907.01372">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhao2019shallow">27</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Zhao, T.&nbsp;N. Sainath, D.&nbsp;Rybach, D.&nbsp;Bhatia, B.&nbsp;Li, and R.&nbsp;Pang,
  &ldquo;Shallow-fusion end-to-end contextual biasing,&rdquo; in <em>Proc. Interspeech</em>,
  2019.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1209.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="TaraRuoming19">28</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, R.&nbsp;Pang, D.&nbsp;Rybach, Y.&nbsp;He, R.&nbsp;Prabhavalkar, W.&nbsp;Li, M.&nbsp;Visontai,
  Q.&nbsp;Liang, T.&nbsp;Strohman, Y.&nbsp;Wu, I.&nbsp;McGraw, and C.-C. Chiu, &ldquo;Two-pass
  end-to-end speech recognition,&rdquo; in <em>Proc. Interspeech</em>, 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1908.10992">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chiu19longform">29</a>]
</td>
<td class="bibtexitem">
C.-C. Chiu, W.&nbsp;Han, Y.&nbsp;Zhang, R.&nbsp;Pang, S.&nbsp;Kishchenko, P.&nbsp;Nguyen, A.&nbsp;Narayanan,
  H.&nbsp;Liao, S.&nbsp;Zhang, A.&nbsp;Kannan, R.&nbsp;Prabhavalkar, Z.&nbsp;Chen, T.&nbsp;Sainath, and
  Y.&nbsp;Wu, &ldquo;A comparison of end-to-end models for long-form speech
  recognition,&rdquo; 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1911.02242">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="narayanan2019longform">30</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Narayanan, R.&nbsp;Prabhavalkar, C.&nbsp;Chiu, D.&nbsp;Rybach, T.&nbsp;Sainath, and T.&nbsp;Strohman,
  &ldquo;Recognizing long-form speech using streaming end-to-end models,&rdquo; 2019.
[&nbsp;<a href="https://arxiv.org/pdf/1910.11455">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sainath2020">31</a>]
</td>
<td class="bibtexitem">
T.&nbsp;N. Sainath, R.&nbsp;Pang, R.&nbsp;Weiss, Y.&nbsp;He, C.-C. Chiu, and T.&nbsp;Strohman, &ldquo;An
  attention-based joint acoustic and text on-device end-to-end model,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2020.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lu2020">32</a>]
</td>
<td class="bibtexitem">
Z.&nbsp;Lu, L.&nbsp;Cao, Y.&nbsp;Zhang, C.-C. Chiu, and J.&nbsp;Fan, &ldquo;Speech sentiment analysis
  via pre-trained features from end-to-end asr models,&rdquo; in <em>Proc. IEEE
  International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP)</em>, 2020.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="park2020">33</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Park, Y.&nbsp;Zhang, C.-C. Chiu, Y.&nbsp;Chen, B.&nbsp;Li, W.&nbsp;Chan, Q.&nbsp;Le, and Y.&nbsp;Wu,
  &ldquo;Specaugment on large scale datas,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2020.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="e2e2020">34</a>]
</td>
<td class="bibtexitem">
T.&nbsp;Sainath, Y.&nbsp;He, B.&nbsp;Li, A.&nbsp;Narayanan, R.&nbsp;Pang, A.&nbsp;Bruguier, S.&nbsp;yiin Chang,
  W.&nbsp;Li, R.&nbsp;Alvarez, Z.&nbsp;Chen, C.&nbsp;cheng Chiu, D.&nbsp;Garcia, A.&nbsp;Gruenstein, K.&nbsp;Hu,
  M.&nbsp;Jin, A.&nbsp;Kannan, Q.&nbsp;Liang, I.&nbsp;McGraw, C.&nbsp;Peyser, R.&nbsp;Prabhavalkar,
  G.&nbsp;Pundak, D.&nbsp;Rybach, Y.&nbsp;Shangguan, Y.&nbsp;Sheth, T.&nbsp;Strohman, M.&nbsp;Visontai,
  Y.&nbsp;Wu, Y.&nbsp;Zhang, and D.&nbsp;Zhao, &ldquo;A streaming on-device end-to-end model
  surpassing server-side conventional model quality and latency,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2020.

</td>
</tr>
</table>

## Language understanding

<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kannan2018semi">1</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Kannan, K.&nbsp;Chen, D.&nbsp;Jaunzeikare, and A.&nbsp;Rajkomar, &ldquo;Semi-Supervised
  Learning for Information Extraction from Dialogue,&rdquo; in <em>Proc.
  Interspeech</em>, 2018.
[&nbsp;<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1318.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="yavuz2018calcs">2</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Yavuz, C.&nbsp;C. Chiu, P.&nbsp;Nguyen, and Y.&nbsp;Wu, &ldquo;CaLcs: Continuously
  Approximating Longest Common Subsequence for Sequence Level Optimization,&rdquo;
  in <em>Proc. Conference on Empirical Methods in Natural Language Processing
  (EMNLP)</em>, 2018.
[&nbsp;<a href="http://aclweb.org/anthology/D18-1406">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="haghani2018s2p">3</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Haghani, A.&nbsp;Narayanan, M.&nbsp;Bacchiani, G.&nbsp;Chuang, N.&nbsp;Gaur, P.&nbsp;Moreno,
  R.&nbsp;Prabhavalkar, Z.&nbsp;Qu, and A.&nbsp;Waters, &ldquo;From Audio to Semantics: Approaches
  to End-to-End Spoken Language Understanding,&rdquo; in <em>Proc. IEEE Spoken
  Language Technology Workshop (SLT)</em>, 2018.
[&nbsp;<a href="https://arxiv.org/abs/1809.09190">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chen2019smartcompose">4</a>]
</td>
<td class="bibtexitem">
M.&nbsp;X. Chen, B.&nbsp;N. Lee, G.&nbsp;Bansal, Y.&nbsp;Cao, S.&nbsp;Zhang, J.&nbsp;Lu, J.&nbsp;Tsay, Y.&nbsp;Wang,
  A.&nbsp;M. Dai, Z.&nbsp;Chen, T.&nbsp;Sohn, and Y.&nbsp;Wu, &ldquo;Gmail smart compose: Real-time
  assisted writing,&rdquo; in <em>Proceedings of the 25th ACM SIGKDD International
  Conference on Knowledge Discovery & Data Mining</em>, Association for Computing
  Machinery, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1906.00080">pdf</a>&nbsp;| 
<a href="https://dl.acm.org/doi/10.1145/3292500.3330723">http</a>&nbsp;]

</td>
</tr>
</table>

## Speech synthesis

<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wang2017tacotron">1</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Wang, R.&nbsp;Skerry-Ryan, D.&nbsp;Stanton, Y.&nbsp;Wu, R.&nbsp;Weiss, N.&nbsp;Jaitly, Z.&nbsp;Yang,
  Y.&nbsp;Xiao, Z.&nbsp;Chen, S.&nbsp;Bengio, Q.&nbsp;Le, Y.&nbsp;Agiomyrgiannakis, R.&nbsp;Clark, and
  R.&nbsp;A. Saurous, &ldquo;Tacotron: Towards end-to-end speech synthesis,&rdquo; in <em>Proc. Interspeech</em>, 2017.
[&nbsp;<a href="https://google.github.io/tacotron/publications/tacotron/index.html">sound examples</a>&nbsp;|
<a href="https://arxiv.org/abs/1703.10135">pdf</a>&nbsp;]

</td>
</tr>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="shen2018natural">1</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Shen, R.&nbsp;Pang, R.&nbsp;J. Weiss, M.&nbsp;Schuster, N.&nbsp;Jaitly, Z.&nbsp;Yang, Z.&nbsp;Chen,
  Y.&nbsp;Zhang, Y.&nbsp;Wang, R.&nbsp;Skerry-Ryan, R.&nbsp;A. Saurous, Y.&nbsp;Agiomyrgiannakis, and
  Y.&nbsp;Wu, &ldquo;Natural TTS synthesis by conditioning WaveNet on mel spectrogram
  predictions,&rdquo; in <em>Proc. IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://google.github.io/tacotron/publications/tacotron2/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1712.05884">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chorowski2018styletransfer">2</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Chorowski, R.&nbsp;J. Weiss, R.&nbsp;A. Saurous, and S.&nbsp;Bengio, &ldquo;On using
  backpropagation for speech texture generation and voice conversion,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2018.
[&nbsp;<a href="https://google.github.io/speech_style_transfer/samples.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1712.08363">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jia2018multispeaker">3</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Jia, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Q.&nbsp;Wang, J.&nbsp;Shen, F.&nbsp;Ren, Z.&nbsp;Chen, P.&nbsp;Nguyen,
  R.&nbsp;Pang, I.&nbsp;Lopez-Moreno, and Y.&nbsp;Wu, &ldquo;Transfer learning from speaker
  verification to multispeaker text-to-speech synthesis,&rdquo; in <em>Advances in
  Neural Information Processing Systems</em>, 2018.
[&nbsp;<a href="https://google.github.io/tacotron/publications/speaker_adaptation/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1806.04558">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hsu2019hierarchical">4</a>]
</td>
<td class="bibtexitem">
W.&nbsp;N. Hsu, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, H.&nbsp;Zen, Y.&nbsp;Wu, Y.&nbsp;Wang, Y.&nbsp;Cao, Y.&nbsp;Jia,
  Z.&nbsp;Chen, J.&nbsp;Shen, P.&nbsp;Nguyen, and R.&nbsp;Pang, &ldquo;Hierarchical generative modeling
  for controllable speech synthesis,&rdquo; in <em>Proc. International Conference
  on Learning Representations (ICLR)</em>, 2019.
[&nbsp;<a href="https://google.github.io/tacotron/publications/gmvae_controllable_tts/index.html">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1810.07217">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hsu2018disentangling">5</a>]
</td>
<td class="bibtexitem">
W.&nbsp;N. Hsu, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;A. Chung, Y.&nbsp;Wang, Y.&nbsp;Wu, and J.&nbsp;Glass,
  &ldquo;Disentangling correlated speaker and noise for speech synthesis via data
  augmentation and adversarial factorization,&rdquo; in <em>NeurIPS 2018 Workshop
  on Interpretability and Robustness in Audio, Speech, and Language</em>, 2018.
[&nbsp;<a href="https://openreview.net/forum?id=Bkg9ZeBB37">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zen2019libritts">6</a>]
</td>
<td class="bibtexitem">
H.&nbsp;Zen, V.&nbsp;Dang, R.&nbsp;Clark, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;Jia, Z.&nbsp;Chen, and Y.&nbsp;Wu,
  &ldquo;LibriTTS: A corpus derived from LibriSpeech for text-to-speech,&rdquo; in
  <em>Proc. Interspeech</em>, 2019.
[&nbsp;<a href="http://www.openslr.org/60">data</a>&nbsp;| 
<a href="https://arxiv.org/abs/1904.02882">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="biadsy2019parrotron">7</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Biadsy, R.&nbsp;J. Weiss, P.&nbsp;Moreno, D.&nbsp;Kanvesky, and Y.&nbsp;Jia, &ldquo;Parrotron: An
  end-to-end speech-to-speech conversion model and its applications to
  hearing-impaired speech and speech separation,&rdquo; in <em>Proc. Interspeech</em>,
  2019.
[&nbsp;<a href="https://google.github.io/tacotron/publications/parrotron">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1904.04169">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhang2019learning">8</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Zhang, R.&nbsp;J. Weiss, H.&nbsp;Zen, Y.&nbsp;Wu, Z.&nbsp;Chen, R.&nbsp;J. Skerry-Ryan, Y.&nbsp;Jia,
  A.&nbsp;Rosenberg, and B.&nbsp;Ramabhadran, &ldquo;Learning to speak fluently in a foreign
  language: Multilingual speech synthesis and cross-language voice cloning,&rdquo;
  in <em>Proc. Interspeech</em>, 2019.
[&nbsp;<a href="http://google.github.io/tacotron/publications/multilingual">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1907.04448">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sun2020finegrained">9</a>]
</td>
<td class="bibtexitem">
G.&nbsp;Sun, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;Cao, H.&nbsp;Zen, and Y.&nbsp;Wu, &ldquo;Fully-hierarchical
  fine-grained prosody modeling for interpretable speech synthesis,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2020.
[&nbsp;<a href="https://google.github.io/tacotron/publications/hierarchical_prosody">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/2002.03785">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sun2020prosodyprior">10</a>]
</td>
<td class="bibtexitem">
G.&nbsp;Sun, Y.&nbsp;Zhang, R.&nbsp;J. Weiss, Y.&nbsp;Cao, H.&nbsp;Zen, A.&nbsp;Rosenberg, B.&nbsp;Ramabhadran,
  and Y.&nbsp;Wu, &ldquo;Generating diverse and natural text-to-speech samples using a
  quantized fine-grained VAE and auto-regressive prosody prior,&rdquo; in <em>
  Proc. IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)</em>, 2020.
[&nbsp;<a href="https://google.github.io/tacotron/publications/prosody_prior">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/2002.03788">pdf</a>&nbsp;]

</td>
</tr>
</table>

## Speech translation

<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -s ieeetr -nodoc -nobibsource -nofooter -nf pdf pdf -nf data data -nf sound_examples "sound examples"  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="weiss2017sequence">1</a>]
</td>
<td class="bibtexitem">
R.&nbsp;J. Weiss, J.&nbsp;Chorowski, N.&nbsp;Jaitly, Y.&nbsp;Wu, and Z.&nbsp;Chen,
  &ldquo;Sequence-to-sequence models can directly translate foreign speech,&rdquo; in
  <em>Proc. Interspeech</em>, 2017.
[&nbsp;<a href="https://arxiv.org/abs/1703.08581">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jia2019leveraging">2</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Jia, M.&nbsp;Johnson, W.&nbsp;Macherey, R.&nbsp;J. Weiss, Y.&nbsp;Cao, C.&nbsp;C. Chiu, N.&nbsp;Ari,
  S.&nbsp;Laurenzo, and Y.&nbsp;Wu, &ldquo;Leveraging weakly supervised data to improve
  end-to-end speech-to-text translation,&rdquo; in <em>Proc. IEEE International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
[&nbsp;<a href="https://arxiv.org/abs/1811.02050">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jia2019direct">3</a>]
</td>
<td class="bibtexitem">
Y.&nbsp;Jia, R.&nbsp;J. Weiss, F.&nbsp;Biadsy, W.&nbsp;Macherey, M.&nbsp;Johnson, Z.&nbsp;Chen, and Y.&nbsp;Wu,
  &ldquo;Direct speech-to-speech translation with a sequence-to-sequence model,&rdquo; in
  <em>Proc. Interspeech</em>, 2019.
[&nbsp;<a href="https://google-research.github.io/lingvo-lab/translatotron">sound examples</a>&nbsp;| 
<a href="https://arxiv.org/abs/1904.06037">pdf</a>&nbsp;]

</td>
</tr>
</table>
