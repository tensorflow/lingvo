

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lingvo.core.py_utils &mdash; Lingvo  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Lingvo
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lingvo.html">lingvo package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Lingvo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>lingvo.core.py_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for lingvo.core.py_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># Lint as: python2, python3</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1"># Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Common utilities.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span> <span class="k">as</span> <span class="nn">py_collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pkgutil</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">import</span> <span class="nn">lingvo.compat</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">hyperparams</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">retry</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">symbolic</span>
<span class="kn">from</span> <span class="nn">lingvo.core</span> <span class="kn">import</span> <span class="n">tshape</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">zip</span>

<span class="kn">from</span> <span class="nn">model_pruning.python</span> <span class="kn">import</span> <span class="n">pruning</span>
<span class="c1"># pylint: disable=g-direct-tensorflow-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="kn">import</span> <span class="n">rewriter_config_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">init_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.tpu</span> <span class="kn">import</span> <span class="n">tpu_function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">deprecation</span>
<span class="c1"># pylint: enable=g-direct-tensorflow-import</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;If False, we disable all asserts.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;enable_check_numerics&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;If False, we bypass calls to CheckNumerics.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span><span class="s1">&#39;print_debug_tensors&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                     <span class="s1">&#39;Whether to print debug tensors.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span>
    <span class="s1">&#39;xla_device&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;If non-empty, can be cpu, gpu, or tpu (case sensitive)&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;use_resource_var&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;Use ResourceVariable instead of RefVariable; this option is &#39;</span>
    <span class="s1">&#39;enabled by default and will be removed in the future.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;tpu_compatible&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;Create variables in a way compatible with TPU. &#39;</span>
    <span class="s1">&#39;This should be true for any job that will interact &#39;</span>
    <span class="s1">&#39;with variables or a checkpoint that will be produced &#39;</span>
    <span class="s1">&#39;or consumed by TPU&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;pin_vars_to_cpu&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;Pin variables to cpu:0.  This is useful for weight-sharing / multi-core &#39;</span>
    <span class="s1">&#39;inference on TPUs in which TPU core variables are managed via &#39;</span>
    <span class="s1">&#39;TPUPartitionedCallOp.&#39;</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s1">&#39;no_identity_on_vars&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;Do not add tf.identity() on vars. This allows TPUPartitionedCallOp to use&#39;</span>
    <span class="s1">&#39;variable handles directly for weight-sharing / multi-core &#39;</span>
    <span class="s1">&#39;inference on TPUs.&#39;</span><span class="p">)</span>

<span class="c1"># NOTE: Using absl flags in libraries are frowned upon for several reasons:</span>
<span class="c1">#</span>
<span class="c1"># 1) They require app.run() or explicit flag parsing, preventing the use of</span>
<span class="c1"># these libraries in environments that don&#39;t look like normal binaries (colab</span>
<span class="c1"># notebooks).</span>
<span class="c1">#</span>
<span class="c1"># 2) They are process-level globals that cannot be scoped or configured except</span>
<span class="c1"># once during binary startup.</span>
<span class="c1">#</span>
<span class="c1"># Because py_utils is a library, no more flags should be used in this file; the</span>
<span class="c1"># existing flags are present for backwards compatibility.  Instead, consider</span>
<span class="c1"># using a stack-scoped configuration object such as the Cluster object. We guard</span>
<span class="c1"># against issue 1 above by using _FromGlobal below, which uses the default value</span>
<span class="c1"># of the FLAG even if flags are unparsed.</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>


<div class="viewcode-block" id="_FromGlobal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._FromGlobal">[docs]</a><span class="k">def</span> <span class="nf">_FromGlobal</span><span class="p">(</span><span class="n">field_name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get &#39;field_name&#39; from a global configuration object.</span>

<span class="sd">  Currently the global configuration object used is FLAGS, but this may</span>
<span class="sd">  change to Cluster() or an equivalent stack-scoped config object.</span>

<span class="sd">  Args:</span>
<span class="sd">    field_name: The string field name to look up.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The value associated with the global configuration string &#39;field_name&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(b/145831327): check the field name in the current cluster object.</span>
  <span class="c1"># If explicitly set, use that value instead of using the FLAG value.</span>

  <span class="c1"># Now check the FLAGS object for backwards compatibility.</span>
  <span class="c1">#</span>
  <span class="c1"># If not explicitly set, get the field from the FLAGS object.  If FLAGS</span>
  <span class="c1"># have not been parsed yet, the default value of the flag will be used.</span>
  <span class="k">return</span> <span class="n">FLAGS</span><span class="p">[</span><span class="n">field_name</span><span class="p">]</span><span class="o">.</span><span class="n">value</span></div>


<span class="n">ENQUEUE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_enqueue_ops&#39;</span>

<span class="n">TPU_EMBEDDING_LOAD_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_tpu_embedding_load_ops&#39;</span>
<span class="n">TPU_EMBEDDING_RETRIEVE_OPS</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_tpu_embedding_retrieve_ops&#39;</span>
<span class="n">TPU_EMBEDDING</span> <span class="o">=</span> <span class="s1">&#39;__tpu_embedding&#39;</span>
<span class="n">TPU_EMBEDDING_ACTIVATIONS</span> <span class="o">=</span> <span class="s1">&#39;__tpu_embedding_activations&#39;</span>

<span class="c1"># pylint: disable=protected-access</span>
<span class="n">deprecation</span><span class="o">.</span><span class="n">_PRINT_DEPRECATION_WARNINGS</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># pylint: enable=protected-access</span>


<div class="viewcode-block" id="Assert"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Assert">[docs]</a><span class="k">def</span> <span class="nf">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_greater_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_greater_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_greater"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_greater">[docs]</a><span class="k">def</span> <span class="nf">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_greater</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_less_equal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_less_equal">[docs]</a><span class="k">def</span> <span class="nf">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_less_equal</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_less"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_less">[docs]</a><span class="k">def</span> <span class="nf">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_less</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_between"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_between">[docs]</a><span class="k">def</span> <span class="nf">assert_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>
      <span class="n">Assert</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">)),</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
      <span class="n">Assert</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">)),</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span></div>


<div class="viewcode-block" id="assert_shape_match"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_shape_match">[docs]</a><span class="k">def</span> <span class="nf">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;msg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
        <span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_same_dim0"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_same_dim0">[docs]</a><span class="k">def</span> <span class="nf">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">assert_same_dim0</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span></div>


<div class="viewcode-block" id="assert_even_divide"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.assert_even_divide">[docs]</a><span class="k">def</span> <span class="nf">assert_even_divide</span><span class="p">(</span><span class="n">denorm</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sd">&quot;&quot;&quot;Asserts that denorm is evenly divided by num.&quot;&quot;&quot;</span>
  <span class="n">denorm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">denorm</span><span class="p">)</span>
  <span class="n">num</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">denorm</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;denorminator.dtype is not tf.int32 or tf.int64.&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">num</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;numerator.dtype is not tf.int32 or tf.int64.&#39;</span><span class="p">)</span>

  <span class="n">num</span> <span class="o">=</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">denorm</span><span class="p">))</span>

  <span class="n">quo</span> <span class="o">=</span> <span class="n">denorm</span> <span class="o">//</span> <span class="n">num</span>
  <span class="k">return</span> <span class="n">assert_equal</span><span class="p">(</span><span class="n">quo</span> <span class="o">*</span> <span class="n">num</span><span class="p">,</span> <span class="n">denorm</span><span class="p">)</span></div>


<div class="viewcode-block" id="_CheckNumerics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CheckNumerics">[docs]</a><span class="k">def</span> <span class="nf">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
    <span class="k">if</span> <span class="s1">&#39;name&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;:\d+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_CheckNumerics&#39;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">check_numerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span> <span class="k">if</span> <span class="n">message</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="CheckNumerics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CheckNumerics">[docs]</a><span class="k">def</span> <span class="nf">CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Check numerics for tensors in inp.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_check_numerics&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">inp</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_CheckNumerics</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="with_dependencies"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.with_dependencies">[docs]</a><span class="k">def</span> <span class="nf">with_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">dependencies</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="_PrintOptions"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._PrintOptions">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_PrintOptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="n">original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">get_printoptions</span><span class="p">()</span>
  <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="o">**</span><span class="n">original</span><span class="p">)</span></div>


<div class="viewcode-block" id="_Print"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Print">[docs]</a><span class="k">def</span> <span class="nf">_Print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">_PrintOptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> = </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array_repr</span><span class="p">(</span><span class="n">x</span><span class="p">))</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Log">[docs]</a><span class="k">def</span> <span class="nf">Log</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Prints out values of tensors.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Log(z, &#39;debug compute()&#39;, x=x, y=y)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Log happens after this tensor&#39;s computed.</span>
<span class="sd">    prefix: Every tensor is logged with this prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Ensures tensors are printed in order.</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Print</span><span class="p">,</span> <span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39; : &#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="Debug"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Debug">[docs]</a><span class="k">def</span> <span class="nf">Debug</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">more</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wrapper around tf.Print() and tf.logging.info() to simplify debug printing.</span>

<span class="sd">  x = py_utils.Debug(x)</span>

<span class="sd">  When the graph is built a regular log info line will be printed:</span>
<span class="sd">  -DBG- py_utils_test.py:429 x=Tensor(...</span>

<span class="sd">  Then when the tensor node is evaluated it will print lines like:</span>
<span class="sd">  -DBG- py_utils_test.py:429 x Const:0[shape=][2 2][x=][[1 2][3 4]]</span>

<span class="sd">  WARNING: The code that parses local variable names can fail. E.g. don&#39;t write</span>
<span class="sd">  two Debug() calls on one line.</span>

<span class="sd">  Args:</span>
<span class="sd">     tensor: A tensor to print.</span>
<span class="sd">     message: A message to print.</span>
<span class="sd">     enabled: To enable the debugging.</span>
<span class="sd">     summarize: Integer with number of tensor values to print.</span>
<span class="sd">     more: An optional list of additional tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">enabled</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>

  <span class="k">if</span> <span class="n">more</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">more</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">caller</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getframeinfo</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">stack</span><span class="p">()[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

  <span class="n">caller_var</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
  <span class="n">caller_more_vars</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">:</span>
    <span class="c1"># Rough and likely to fail. But better than nothing.</span>
    <span class="n">caller_var</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Debug\((.*?)(\)|,).*$&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">search</span><span class="p">(</span>
        <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">more</span><span class="p">:</span>
      <span class="n">more_vars</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;more=\[(.*?)\].*$&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">search</span><span class="p">(</span>
          <span class="n">caller</span><span class="o">.</span><span class="n">code_context</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">caller_more_vars</span> <span class="o">=</span> <span class="n">more_vars</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

  <span class="n">header</span> <span class="o">=</span> <span class="s1">&#39;-DBG- </span><span class="si">{}</span><span class="s1">:</span><span class="si">{}</span><span class="s1"> </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
      <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">caller</span><span class="o">.</span><span class="n">filename</span><span class="p">),</span> <span class="n">caller</span><span class="o">.</span><span class="n">lineno</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

  <span class="n">info</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1">=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">header</span><span class="p">,</span> <span class="n">caller_var</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">caller_more_vars</span><span class="p">,</span> <span class="n">more</span><span class="p">):</span>
    <span class="n">info</span> <span class="o">+=</span> <span class="s1">&#39; </span><span class="si">{}</span><span class="s1">=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">val</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;shape=&#39;</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">caller_var</span><span class="p">)),</span> <span class="n">tensor</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">caller_more_vars</span><span class="p">,</span> <span class="n">more</span><span class="p">):</span>
      <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">=&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">strip</span><span class="p">())))</span>
      <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="n">info</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}{}</span><span class="s1"> </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">header</span><span class="p">,</span> <span class="n">caller_var</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Print</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="_Save"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Save">[docs]</a><span class="k">def</span> <span class="nf">_Save</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.</span><span class="si">%08d</span><span class="s1">.</span><span class="si">%s</span><span class="s1">.npy&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">ensure_text</span><span class="p">(</span><span class="n">prefix</span><span class="p">),</span> <span class="n">steps</span><span class="p">,</span>
                                 <span class="n">six</span><span class="o">.</span><span class="n">ensure_text</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span></div>


<div class="viewcode-block" id="Save"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Save">[docs]</a><span class="k">def</span> <span class="nf">Save</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Saves values of tensors into files.</span>

<span class="sd">  Useful for debugging. E.g.,</span>
<span class="sd">    x = ... a tf.Tensor ...</span>
<span class="sd">    y = ... a tf.Tensor ...</span>
<span class="sd">    z = compute(x, y)</span>
<span class="sd">    z = Save(z, &#39;/path/tmp&#39;, x=x, y=y, z=z)</span>

<span class="sd">  Args:</span>
<span class="sd">    value: A Tensor. Saving happens after this tensor is computed.</span>
<span class="sd">    filename_prefix: Every tensor is saved with this filename prefix.</span>
<span class="sd">    **kwargs: keywords and tensors. Tensors are logged in the sort order of</span>
<span class="sd">      these keywards.</span>

<span class="sd">  Returns:</span>
<span class="sd">    value is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">last</span> <span class="o">=</span> <span class="n">value</span>
  <span class="n">steps</span> <span class="o">=</span> <span class="n">GetGlobalStep</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
      <span class="n">last</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_Save</span><span class="p">,</span> <span class="p">[</span><span class="n">steps</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="p">[])</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">last</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="HasRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasRank">[docs]</a><span class="k">def</span> <span class="nf">HasRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected rank.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="n">expected_rank</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Ranks did not match, got </span><span class="si">%d</span><span class="s1">, &#39;</span>
        <span class="s1">&#39;expected </span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">expected_rank</span><span class="p">)],</span>
                             <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="HasAtLeastRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasAtLeastRank">[docs]</a><span class="k">def</span> <span class="nf">HasAtLeastRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has rank &gt;= expected_rank.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expected_rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">&gt;=</span> <span class="n">expected_rank</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Rank of tensor </span><span class="si">%d</span><span class="s1"> did not exceed the expected value </span><span class="si">%d</span><span class="s1">.&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">expected_rank</span><span class="p">)],</span> <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="GetRank"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetRank">[docs]</a><span class="k">def</span> <span class="nf">GetRank</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns tensor&#39;s rank as an int if it&#39;s available, otherwise a Tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: The input tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Either an int or a Tensor for the rank of the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>  <span class="c1"># int</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>  <span class="c1"># Tensor</span></div>


<div class="viewcode-block" id="HasShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasShape">[docs]</a><span class="k">def</span> <span class="nf">HasShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Syntactic sugar for asserting that tensor has the expected shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A Tensor.</span>
<span class="sd">    expected_shape: A Python list or a 1D tensor.</span>
<span class="sd">    ndims: If not None, check only the first `ndims` dimensions of `tensor`.</span>
<span class="sd">      Must be equal to the length of `expected_shape` if not None.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The input `tensor`</span>
<span class="sd">  Raises:</span>
<span class="sd">    A runtime error if the assertion fails.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">):</span>
    <span class="n">filepath</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">extract_stack</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">3</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;LINGVO ASSERT </span><span class="si">%s</span><span class="s1">:</span><span class="si">%s</span><span class="s1">(</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;.*/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                                 <span class="n">filepath</span><span class="p">),</span> <span class="n">line</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">with_dependencies</span><span class="p">([</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">assert_shape_match</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[:</span><span class="n">ndims</span><span class="p">],</span> <span class="n">expected_shape</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">tensor</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="GetShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetShape">[docs]</a><span class="k">def</span> <span class="nf">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ndims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns tensor&#39;s shape as a list which can be unpacked, unlike tf.shape.</span>

<span class="sd">  Tries to return static shape if it&#39;s available. Note that this means</span>
<span class="sd">  some of the outputs will be ints while the rest will be Tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: The input tensor.</span>
<span class="sd">    ndims: If not None, returns the shapes for the first `ndims` dimensions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">dynamic_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

  <span class="c1"># Early exit for unranked tensor.</span>
  <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">dynamic_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)]</span>

  <span class="c1"># Ranked tensor.</span>
  <span class="k">if</span> <span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ndims</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ndims</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span><span class="p">)</span>

  <span class="c1"># Return mixture of static and dynamic dims.</span>
  <span class="n">static_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">if</span> <span class="n">static_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dynamic_shape</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndims</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="k">return</span> <span class="n">shapes</span></div>


<div class="viewcode-block" id="GetSize"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetSize">[docs]</a><span class="k">def</span> <span class="nf">GetSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span>
      <span class="nb">any</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">])):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="use_xla"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.use_xla">[docs]</a><span class="k">def</span> <span class="nf">use_xla</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;xla_device&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">res</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">res</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu&#39;</span><span class="p">,</span> <span class="s1">&#39;tpu&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="use_tpu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.use_tpu">[docs]</a><span class="k">def</span> <span class="nf">use_tpu</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="n">res</span> <span class="o">=</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;xla_device&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;tpu&#39;</span>
  <span class="k">if</span> <span class="n">res</span><span class="p">:</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;enable_asserts&#39;</span><span class="p">)</span>  <span class="c1"># asserts not supported on tpu</span>
  <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="tpu_compat"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.tpu_compat">[docs]</a><span class="k">def</span> <span class="nf">tpu_compat</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="ow">or</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;tpu_compatible&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="use_resource_variables"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.use_resource_variables">[docs]</a><span class="k">def</span> <span class="nf">use_resource_variables</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">return</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;use_resource_var&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">tpu_compat</span><span class="p">()</span></div>


<div class="viewcode-block" id="outside_all_rewrites"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.outside_all_rewrites">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">outside_all_rewrites</span><span class="p">():</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">yield</span></div>


<div class="viewcode-block" id="_ThreadLocalStack"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._ThreadLocalStack">[docs]</a><span class="k">class</span> <span class="nc">_ThreadLocalStack</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[]</span></div>


<span class="c1"># TODO(jamesqin): remove once b/147439702 is fixed.</span>
<span class="n">_OUTSIDE_COMPILATION</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>


<div class="viewcode-block" id="RunOnTpuHost"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RunOnTpuHost">[docs]</a><span class="k">def</span> <span class="nf">RunOnTpuHost</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Runs the given function call on TPU host.</span>

<span class="sd">  Invokes func(\*args, \*\*kwargs) directly if not running on tpu.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to invoke.</span>
<span class="sd">    *args: args of func</span>
<span class="sd">    **kwargs: kwargs of func</span>

<span class="sd">  Returns:</span>
<span class="sd">    The function return value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_OUTSIDE_COMPILATION</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
    <span class="n">_OUTSIDE_COMPILATION</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">outside_compilation</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">_OUTSIDE_COMPILATION</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="tpu_host"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.tpu_host">[docs]</a><span class="k">def</span> <span class="nf">tpu_host</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Decorates a python function to only run on TPU hosts.</span>

<span class="sd">  This function has no effect when running on CPU/GPU.</span>

<span class="sd">  Example::</span>

<span class="sd">    @py_utils.tpu_host()</span>
<span class="sd">    def ComputeWER(self):</span>
<span class="sd">      # Call a custom op computing WER.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: the function to invoke</span>

<span class="sd">  Returns:</span>
<span class="sd">    A TPU-host only function</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">RunOnTpuHost</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Wrapped</span></div>


<span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="SetTpuDeviceAssignment"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SetTpuDeviceAssignment">[docs]</a><span class="k">def</span> <span class="nf">SetTpuDeviceAssignment</span><span class="p">(</span><span class="n">tpu_device_assignment</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">_tpu_device_assignment</span>
  <span class="k">if</span> <span class="n">_tpu_device_assignment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;tpu_device_assignment was already set, &#39;</span>
                       <span class="s1">&#39;overwriting with new assignment.&#39;</span><span class="p">)</span>
  <span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="n">tpu_device_assignment</span></div>


<span class="c1"># This function should called in unittest only.</span>
<div class="viewcode-block" id="ClearTpuDevice"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ClearTpuDevice">[docs]</a><span class="k">def</span> <span class="nf">ClearTpuDevice</span><span class="p">():</span>
  <span class="k">global</span> <span class="n">_tpu_device_assignment</span>
  <span class="n">_tpu_device_assignment</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="GetTpuDeviceAssignment"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetTpuDeviceAssignment">[docs]</a><span class="k">def</span> <span class="nf">GetTpuDeviceAssignment</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">_tpu_device_assignment</span></div>


<div class="viewcode-block" id="SessionConfig"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SessionConfig">[docs]</a><span class="k">def</span> <span class="nf">SessionConfig</span><span class="p">(</span><span class="n">soft_placement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cluster_def</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a session config proto.</span>

<span class="sd">  Args:</span>
<span class="sd">    soft_placement: Turns allow_soft_placement on iff True.</span>
<span class="sd">    inline: Turns do_function_inlining on iff True.</span>
<span class="sd">    cluster_def: A tf.train.ClusterDef describing the cluster.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A TF session config proto.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">session_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span>
      <span class="n">allow_soft_placement</span><span class="o">=</span><span class="n">soft_placement</span><span class="p">,</span>
      <span class="n">graph_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphOptions</span><span class="p">(</span>
          <span class="n">optimizer_options</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="p">(</span>
              <span class="n">opt_level</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">OptimizerOptions</span><span class="o">.</span><span class="n">L1</span><span class="p">,</span> <span class="n">do_function_inlining</span><span class="o">=</span><span class="n">inline</span><span class="p">)),</span>
      <span class="n">cluster_def</span><span class="o">=</span><span class="n">cluster_def</span><span class="p">)</span>
  <span class="c1"># Disable layout optimizer which increases GPU memory usage.</span>
  <span class="n">session_config</span><span class="o">.</span><span class="n">graph_options</span><span class="o">.</span><span class="n">rewrite_options</span><span class="o">.</span><span class="n">layout_optimizer</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">rewriter_config_pb2</span><span class="o">.</span><span class="n">RewriterConfig</span><span class="o">.</span><span class="n">OFF</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">session_config</span></div>


<div class="viewcode-block" id="AssertIsCompatible"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AssertIsCompatible">[docs]</a><span class="k">def</span> <span class="nf">AssertIsCompatible</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">IsCompatible</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> vs </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span></div>


<div class="viewcode-block" id="SetShapes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SetShapes">[docs]</a><span class="k">def</span> <span class="nf">SetShapes</span><span class="p">(</span><span class="n">dst_nmap</span><span class="p">,</span> <span class="n">src_nmap</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Set shapes in dst_nmap using those in src_nmap.&quot;&quot;&quot;</span>
  <span class="n">AssertIsCompatible</span><span class="p">(</span><span class="n">src_nmap</span><span class="p">,</span> <span class="n">dst_nmap</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">dst</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">src_nmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">dst_nmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()):</span>
    <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dtypes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Dtypes">[docs]</a><span class="k">def</span> <span class="nf">Dtypes</span><span class="p">(</span><span class="n">nmap_list</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns all tensors&#39; data types in a list.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">nmap_list</span><span class="p">)]</span></div>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Flatten">[docs]</a><span class="k">def</span> <span class="nf">Flatten</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Flattens &#39;x&#39; by extracting tensors from nested structures to a list.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="Pack"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Pack">[docs]</a><span class="k">def</span> <span class="nf">Pack</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Packs &#39;values&#39; according to &#39;tmpl&#39;.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span></div>


<div class="viewcode-block" id="Transform"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Transform">[docs]</a><span class="k">def</span> <span class="nf">Transform</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">v</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replaces every nested value x in &#39;v&#39; with fn(x) and returns the result.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="IsCompatible"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.IsCompatible">[docs]</a><span class="k">def</span> <span class="nf">IsCompatible</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns true if lhs and rhs are compatible.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">False</span></div>


<span class="n">_NAME_PATTERN</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;[A-Za-z_][A-Za-z0-9_]*&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="NestedMap"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap">[docs]</a><span class="k">class</span> <span class="nc">NestedMap</span><span class="p">(</span><span class="nb">dict</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A simple helper to maintain a dict.</span>

<span class="sd">  It is a sub-class of dict with the following extensions/restrictions:</span>
<span class="sd">    - It supports attr access to its members (see examples below).</span>
<span class="sd">    - Member keys have to be valid identifiers.</span>

<span class="sd">  E.g.::</span>

<span class="sd">      &gt;&gt;&gt; foo = NestedMap()</span>
<span class="sd">      &gt;&gt;&gt; foo[&#39;x&#39;] = 10</span>
<span class="sd">      &gt;&gt;&gt; foo.y = 20</span>
<span class="sd">      &gt;&gt;&gt; assert foo.x * 2 == foo.y</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Disable pytype attribute checking.</span>
  <span class="n">_HAS_DYNAMIC_ATTRIBUTES</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="c1"># keys in this list are not allowed in a NestedMap.</span>
  <span class="n">_RESERVED_KEYS</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">dir</span><span class="p">(</span><span class="nb">dict</span><span class="p">))</span>
  <span class="c1"># sentinel value for deleting keys used in Filter.</span>
  <span class="n">_DELETE</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NestedMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">),</span> <span class="p">(</span>
          <span class="s1">&#39;Key in a NestedMap has to be a six.string_types. Currently type: </span><span class="si">%s</span><span class="s1">,&#39;</span>
          <span class="s1">&#39; value: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">key</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)))</span>
      <span class="n">NestedMap</span><span class="o">.</span><span class="n">CheckKey</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
      <span class="k">assert</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">NestedMap</span><span class="o">.</span><span class="n">_RESERVED_KEYS</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is a reserved key&#39;</span> <span class="o">%</span> <span class="n">key</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="c1"># Make sure key is a valid expression and is not one of the reserved</span>
    <span class="c1"># attributes.</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">),</span> <span class="p">(</span>
        <span class="s1">&#39;Key in a NestedMap has to be a six.string_types. Currently type: </span><span class="si">%s</span><span class="s1">, &#39;</span>
        <span class="s1">&#39;value: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">key</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)))</span>
    <span class="n">NestedMap</span><span class="o">.</span><span class="n">CheckKey</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">NestedMap</span><span class="o">.</span><span class="n">_RESERVED_KEYS</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is a reserved key&#39;</span> <span class="o">%</span> <span class="n">key</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NestedMap</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setitem__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="fm">__setitem__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">; available attributes: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()))))</span>

  <span class="k">def</span> <span class="fm">__delattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">del</span> <span class="bp">self</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">; available attributes: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                           <span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()))))</span>

<div class="viewcode-block" id="NestedMap.copy"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.copy">[docs]</a>  <span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># Don&#39;t delegate w/ super: dict.copy() -&gt; dict.</span>
    <span class="k">return</span> <span class="n">NestedMap</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_memo</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deep-copies the structure but not the leaf objects.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">DeepCopy</span><span class="p">()</span>

<div class="viewcode-block" id="NestedMap.DeepCopy"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.DeepCopy">[docs]</a>  <span class="k">def</span> <span class="nf">DeepCopy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deep-copies the structure but not the leaf objects.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span></div>

<div class="viewcode-block" id="NestedMap.FromNestedDict"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.FromNestedDict">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">FromNestedDict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts every dict in nested structure &#39;x&#39; to a NestedMap.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
      <span class="n">res</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">res</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="o">.</span><span class="n">FromNestedDict</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">res</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)(</span><span class="n">NestedMap</span><span class="o">.</span><span class="n">FromNestedDict</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="NestedMap.CheckKey"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.CheckKey">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">CheckKey</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Asserts that key is valid NestedMap key.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_NAME_PATTERN</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid NestedMap key </span><span class="se">\&#39;</span><span class="si">{}</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span></div>

<div class="viewcode-block" id="NestedMap.GetItem"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.GetItem">[docs]</a>  <span class="k">def</span> <span class="nf">GetItem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the value for the nested `key`.</span>

<span class="sd">    Note that indexing lists is not supported, names with underscores will be</span>
<span class="sd">    considered as one key.</span>

<span class="sd">    Args:</span>
<span class="sd">      key: str of the form</span>
<span class="sd">        `([A-Za-z_][A-Za-z0-9_]*)(.[A-Za-z_][A-Za-z0-9_]*)*.`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The value for the given nested key.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError if a key is not present.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="c1"># Note: This can&#39;t support lists. List keys are ambiguous as underscore is</span>
    <span class="c1"># not reserved for list indexing but also allowed to be used in keys.</span>
    <span class="c1"># E.g., this is a valid nested map where the key &#39;a_0&#39; is not well defined</span>
    <span class="c1"># {&#39;a_0&#39;: 3, &#39;a&#39;: [4]}.</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">):</span>
      <span class="n">current</span> <span class="o">=</span> <span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">current</span></div>

<div class="viewcode-block" id="NestedMap.Get"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Get">[docs]</a>  <span class="k">def</span> <span class="nf">Get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the value for nested `key`, returns `default` if key does not exist.</span>

<span class="sd">    Note that indexing lists is not supported, names with underscores will be</span>
<span class="sd">    considered as one key.</span>

<span class="sd">    Args:</span>
<span class="sd">      key: str of the form</span>
<span class="sd">        `([A-Za-z_][A-Za-z0-9_]*)(.[A-Za-z_][A-Za-z0-9_]*)*.`.</span>
<span class="sd">      default: Optional default value, defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The value for the given nested key or `default` if the key does not exist.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">GetItem</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="c1"># TypeError is raised when an intermediate item is a list and we try to</span>
    <span class="c1"># access an element of it with a string.</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">KeyError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">default</span></div>

<div class="viewcode-block" id="NestedMap.Set"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Set">[docs]</a>  <span class="k">def</span> <span class="nf">Set</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the value for a nested key.</span>

<span class="sd">    Note that indexing lists is not supported, names with underscores will be</span>
<span class="sd">    considered as one key.</span>

<span class="sd">    Args:</span>
<span class="sd">      key: str of the form</span>
<span class="sd">        `([A-Za-z_][A-Za-z0-9_]*)(.[A-Za-z_][A-Za-z0-9_]*)*.`.</span>
<span class="sd">      value: The value to insert.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError if a sub key is not a NestedMap or dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span>
    <span class="n">sub_keys</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sub_keys</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">CheckKey</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
      <span class="c1"># We have reached the terminal node, set the value.</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sub_keys</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current</span><span class="p">:</span>
          <span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Error while setting key </span><span class="si">{}</span><span class="s1">. Sub key &quot;</span><span class="si">{}</span><span class="s1">&quot; is of type&#39;</span>
                           <span class="s1">&#39; </span><span class="si">{}</span><span class="s1"> but must be a dict or NestedMap.&#39;</span>
                           <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">current</span><span class="p">[</span><span class="n">k</span><span class="p">]</span></div>

<div class="viewcode-block" id="NestedMap._RecursiveMap"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap._RecursiveMap">[docs]</a>  <span class="k">def</span> <span class="nf">_RecursiveMap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Traverse recursively into lists and NestedMaps applying `fn`.</span>

<span class="sd">    Args:</span>
<span class="sd">      fn: The function to apply to each item (leaf node).</span>
<span class="sd">      flatten: If true, the result should be a single flat list. Otherwise the</span>
<span class="sd">        result will have the same structure as this NestedMap.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The result of applying fn.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">Recurse</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Helper function for _RecursiveMap.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">flatten</span> <span class="k">else</span> <span class="n">NestedMap</span><span class="p">()</span>
        <span class="n">deleted</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
          <span class="n">res</span> <span class="o">=</span> <span class="n">Recurse</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">k</span> <span class="k">if</span> <span class="n">key</span> <span class="k">else</span> <span class="n">k</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span><span class="p">:</span>
            <span class="n">deleted</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">continue</span>
          <span class="k">elif</span> <span class="n">flatten</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">+=</span> <span class="n">res</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span> <span class="ow">and</span> <span class="n">deleted</span><span class="p">:</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">deleted</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
          <span class="n">res</span> <span class="o">=</span> <span class="n">Recurse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
          <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span><span class="p">:</span>
            <span class="n">deleted</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">continue</span>
          <span class="k">elif</span> <span class="n">flatten</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">+=</span> <span class="n">res</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span> <span class="ow">and</span> <span class="n">deleted</span><span class="p">:</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span>
        <span class="k">return</span> <span class="n">ret</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">flatten</span><span class="p">:</span>
          <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="n">ret</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">Recurse</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">flatten</span> <span class="k">else</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">res</span></div>

<div class="viewcode-block" id="NestedMap.Flatten"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Flatten">[docs]</a>  <span class="k">def</span> <span class="nf">Flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list containing the flattened values in the `.NestedMap`.</span>

<span class="sd">    Unlike py_utils.Flatten(), this will only descend into lists and NestedMaps</span>
<span class="sd">    and not dicts, tuples, or namedtuples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="NestedMap.FlattenItems"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.FlattenItems">[docs]</a>  <span class="k">def</span> <span class="nf">FlattenItems</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten the `.NestedMap` and returns &lt;key, value&gt; pairs in a list.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of &lt;key, value&gt; pairs, where keys for nested entries will be</span>
<span class="sd">      represented in the form of `foo.bar[10].baz`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span> <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="NestedMap.Pack"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Pack">[docs]</a>  <span class="k">def</span> <span class="nf">Pack</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lst</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this with each value replaced by a value in lst.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">())</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
    <span class="n">v_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">unused_k</span><span class="p">,</span> <span class="n">unused_v</span><span class="p">:</span> <span class="nb">next</span><span class="p">(</span><span class="n">v_iter</span><span class="p">))</span></div>

<div class="viewcode-block" id="NestedMap.Transform"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Transform">[docs]</a>  <span class="k">def</span> <span class="nf">Transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this `.NestedMap` with fn applied on each value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">fn</span><span class="p">(</span><span class="n">v</span><span class="p">))</span></div>

<div class="viewcode-block" id="NestedMap.IsCompatible"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.IsCompatible">[docs]</a>  <span class="k">def</span> <span class="nf">IsCompatible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns true if self and other are compatible.</span>

<span class="sd">    If x and y are two compatible `.NestedMap`, `x.Pack(y.Flatten())` produces y</span>
<span class="sd">    and vice versa.</span>

<span class="sd">    Args:</span>
<span class="sd">      other: Another `.NestedMap`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">items</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">other_items</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">k</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">items</span> <span class="o">==</span> <span class="n">other_items</span></div>

<div class="viewcode-block" id="NestedMap.Filter"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.Filter">[docs]</a>  <span class="k">def</span> <span class="nf">Filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy with entries where fn(entry) is True.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FilterKeyVal</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">fn</span><span class="p">(</span><span class="n">v</span><span class="p">))</span></div>

<div class="viewcode-block" id="NestedMap.FilterKeyVal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.FilterKeyVal">[docs]</a>  <span class="k">def</span> <span class="nf">FilterKeyVal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a copy of this `.NestedMap` filtered by fn.</span>

<span class="sd">    If fn(key, entry) is True, the entry is copied into the returned NestedMap.</span>
<span class="sd">    Otherwise, it is not copied.</span>

<span class="sd">    Args:</span>
<span class="sd">      fn: a callable of (string, entry)-&gt;boolean.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `.NestedMap` contains copied entries from this `&#39;.NestedMap`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RecursiveMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span> <span class="k">if</span> <span class="n">fn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_DELETE</span><span class="p">)</span></div>

<div class="viewcode-block" id="NestedMap._ToStrings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap._ToStrings">[docs]</a>  <span class="k">def</span> <span class="nf">_ToStrings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns debug strings in a list for this `.NestedMap`.&quot;&quot;&quot;</span>
    <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">()</span>
    <span class="n">maxlen</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">kv</span><span class="p">])</span> <span class="k">if</span> <span class="n">kv</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">k</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">+</span> <span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">k</span><span class="p">))</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kv</span><span class="p">])</span></div>

<div class="viewcode-block" id="NestedMap.DebugString"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.DebugString">[docs]</a>  <span class="k">def</span> <span class="nf">DebugString</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a debug string for this `.NestedMap`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ToStrings</span><span class="p">())</span></div>

<div class="viewcode-block" id="NestedMap.VLog"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NestedMap.VLog">[docs]</a>  <span class="k">def</span> <span class="nf">VLog</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logs the debug string at the level.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">level</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">level</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">prefix</span> <span class="o">=</span> <span class="s1">&#39;nmap: &#39;</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ToStrings</span><span class="p">():</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="n">level</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="_Unique"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Unique">[docs]</a><span class="k">class</span> <span class="nc">_Unique</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper to uniqify variables in a NestedMap.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_vset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="ToUniqueList"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToUniqueList">[docs]</a><span class="k">def</span> <span class="nf">ToUniqueList</span><span class="p">(</span><span class="n">nmap</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the flattened `nmap` with duplicates removed.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span></div>


<div class="viewcode-block" id="ReadOnlyAttrDictView"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadOnlyAttrDictView">[docs]</a><span class="k">def</span> <span class="nf">ReadOnlyAttrDictView</span><span class="p">(</span><span class="n">backing</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a dict to provide a read-only view of its contents.</span>

<span class="sd">  Dict keys can also be accessed by attribute.</span>

<span class="sd">  Args:</span>
<span class="sd">    backing: Dict-like object to wrap.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Read-only Mapping that can be accessed by index ([&#39;foo&#39;]) or attr (d.foo).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">Wrapper</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrapper object.&quot;&quot;&quot;</span>

    <span class="c1"># Disable pytype attribute checking.</span>
    <span class="n">_HAS_DYNAMIC_ATTRIBUTES</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backing</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">backing</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__hasattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">backing</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Dictionary is read-only.&#39;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">Wrapper</span><span class="p">()</span></div>


<div class="viewcode-block" id="ToStaticShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToStaticShape">[docs]</a><span class="k">def</span> <span class="nf">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts &#39;shape&#39; to a static shape.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span>
    <span class="p">]</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="n">static_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">static_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">static_shape</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">shape</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Dimension</span><span class="p">)</span> <span class="k">else</span> <span class="n">shape</span></div>


<div class="viewcode-block" id="Zeros"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Zeros">[docs]</a><span class="k">def</span> <span class="nf">Zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="UniformSampler"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UniformSampler">[docs]</a><span class="k">class</span> <span class="nc">UniformSampler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A reservoir sampler.</span>

<span class="sd">  This class implements reservoir sampling: Given a limit of `num_samples` total</span>
<span class="sd">  samples, this class maintains a uniform probability (1 / `num_samples`) of</span>
<span class="sd">  keeping any item dynamically added to the sampler.</span>

<span class="sd">  See https://en.wikipedia.org/wiki/Reservoir_sampling for details.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span> <span class="o">=</span> <span class="n">num_samples</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="UniformSampler.Add"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UniformSampler.Add">[docs]</a>  <span class="k">def</span> <span class="nf">Add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add item to sampler.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_seen_items</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_samples</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span></div>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">samples</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fetch the current samples from the sampler.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samples</span></div>


<div class="viewcode-block" id="RNNCellStateInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit">[docs]</a><span class="k">class</span> <span class="nc">RNNCellStateInit</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;State initialization functions for RNN cell init state.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="RNNCellStateInit._Params"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit._Params">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span>
             <span class="s1">&#39;Initialization method. Should be one of zeros, random_normal.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="RNNCellStateInit.Zeros"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit.Zeros">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Zeros</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;tf.zeros().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></div>

<div class="viewcode-block" id="RNNCellStateInit.RandomNormal"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RNNCellStateInit.RandomNormal">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">RandomNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;tf.random.normal().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;random_normal&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DefaultRNNCellStateInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefaultRNNCellStateInit">[docs]</a><span class="k">def</span> <span class="nf">DefaultRNNCellStateInit</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">RNNCellStateInit</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()</span></div>


<div class="viewcode-block" id="InitRNNCellState"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.InitRNNCellState">[docs]</a><span class="k">def</span> <span class="nf">InitRNNCellState</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Initial state definitions for RNN cell implementations.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: A array of ints/symbols for specifying the shape of the state.</span>
<span class="sd">    init: Hyperparameters as returned by one of the static implemetaitons in</span>
<span class="sd">      RNNCellStateInit.</span>
<span class="sd">    dtype: The dype of the states. Defaults to tf.float32.</span>
<span class="sd">    name: An optional name for the operation.</span>
<span class="sd">    is_eval: Bool, set to True if we need special behavior in eval mode.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor of the specified shape, and sampled from the distribution as</span>
<span class="sd">    defined by the init parameters.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">ToStaticShape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">DefaultRNNCellStateInit</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>

  <span class="n">method</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="k">if</span> <span class="p">((</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;zeros&#39;</span><span class="p">])</span> <span class="ow">or</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_eval</span><span class="p">)):</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;random_normal&#39;</span><span class="p">]:</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Initialization method (</span><span class="si">%s</span><span class="s1">) not supported.&#39;</span> <span class="o">%</span> <span class="n">method</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">init_state</span></div>


<div class="viewcode-block" id="WeightInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit">[docs]</a><span class="k">class</span> <span class="nc">WeightInit</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Static class providing weight initialization config params.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="WeightInit._Params"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit._Params">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_Params</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s1">&#39;Initialization scale.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate initial values.&#39;</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">Freeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">p</span></div>

<div class="viewcode-block" id="WeightInit.Gaussian"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Gaussian">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Gaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Uniform"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Uniform">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Uniform</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_uniform(-1.0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformPositive"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformPositive">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformPositive</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_uniform(0., 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Xavier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Xavier">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Xavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.XavierWithFixupParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.XavierWithFixupParams">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">XavierWithFixupParams</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">depth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">layers_per_residual_block</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Xavier initialization with Fixup.&quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">layers_per_residual_block</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GeoMeanXavier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GeoMeanXavier">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GeoMeanXavier</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A variant of Xavier (x = sqrt(3. / sqrt(in * out)); [-x, x]).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.Constant"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.Constant">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">Constant</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussian"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussian">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussian</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1.0).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtFanIn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtFanIn">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.GaussianSqrtFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.GaussianSqrtFanOut">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">GaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.random_normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.uniform(-1 / sqrt(dim0), 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.UniformUnitScaling"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.UniformUnitScaling">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">UniformUnitScaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * sqrt(3) / sqrt(dim0) * tf.uniform(-1, 1).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtDim">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtDim</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(dim0)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtFanIn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtFanIn">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanIn</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(fan_in)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.TruncatedGaussianSqrtFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.TruncatedGaussianSqrtFanOut">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">TruncatedGaussianSqrtFanOut</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;scale * tf.truncated_normal(0, 1 / sqrt(fan_out)).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.KaimingUniformFanInRelu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.KaimingUniformFanInRelu">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="WeightInit.KaimingUniformFanInLeakyRelu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightInit.KaimingUniformFanInLeakyRelu">[docs]</a>  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">KaimingUniformFanInLeakyRelu</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">5.</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">_Params</span><span class="p">(</span><span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div></div>


<span class="n">_DEFAULT_XAVIER_INIT</span> <span class="o">=</span> <span class="mf">1.000001</span>


<div class="viewcode-block" id="DefaultParamInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DefaultParamInit">[docs]</a><span class="k">def</span> <span class="nf">DefaultParamInit</span><span class="p">():</span>
  <span class="c1"># Here we use 1.000001 as a signature for user picking up the</span>
  <span class="c1"># default param initializer.</span>
  <span class="k">return</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span></div>


<div class="viewcode-block" id="IsDefaultParamInit"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.IsDefaultParamInit">[docs]</a><span class="k">def</span> <span class="nf">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="n">_DEFAULT_XAVIER_INIT</span> <span class="ow">and</span>
          <span class="n">p</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="WeightParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightParams">[docs]</a><span class="k">def</span> <span class="nf">WeightParams</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for a weight variable given the shape/init/dtype.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">WeightInit</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">_DEFAULT_XAVIER_INIT</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
  <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">collections</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;The weight data type.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;The weight shape.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;init&#39;</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="s1">&#39;Initialization method.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span>
           <span class="s1">&#39;Variable collections this weight belongs to.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span></div>


<div class="viewcode-block" id="FindNeeded"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FindNeeded">[docs]</a><span class="k">def</span> <span class="nf">FindNeeded</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;List names of tensors and operations required to compute endpoints.&quot;&quot;&quot;</span>
  <span class="n">names_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">queue</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">endpoints</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">names_seen</span><span class="p">:</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="n">names_seen</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">o</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">op</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
      <span class="n">queue</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">names_seen</span></div>


<div class="viewcode-block" id="FindNeededInList"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FindNeededInList">[docs]</a><span class="k">def</span> <span class="nf">FindNeededInList</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">endpoints</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return tensors from tensor_list needed to compute any of endpoints.&quot;&quot;&quot;</span>
  <span class="n">all_needed</span> <span class="o">=</span> <span class="n">FindNeeded</span><span class="p">(</span><span class="n">endpoints</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">all_needed</span><span class="p">]</span></div>


<div class="viewcode-block" id="_CollectionGetter"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._CollectionGetter">[docs]</a><span class="k">class</span> <span class="nc">_CollectionGetter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get graph local value from a defined collection.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">default_factory</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_key</span> <span class="o">=</span> <span class="n">key</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span> <span class="o">=</span> <span class="n">default_factory</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">collection</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collection</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">collection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_factory</span><span class="p">()</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="SanitizeScopeKey"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SanitizeScopeKey">[docs]</a><span class="k">def</span> <span class="nf">SanitizeScopeKey</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Removes invalid symbols from name_scope keys.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;[&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span></div>


<span class="c1"># Global variable to control multitask variable reuse</span>
<span class="c1"># If False (default) the default tf.get_variable is used, that is:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables</span>
<span class="c1"># - Non-reusing scopes only allow getting new variables</span>
<span class="c1"># With OPPORTUNISTIC_VARIABLE_REUSE==True:</span>
<span class="c1"># - Reusing scopes only allow getting existing variables, as usual</span>
<span class="c1"># - Non-reusing scopes reuse new variables or get new ones</span>
<span class="n">_OPPORTUNISTIC_VARIABLE_REUSE_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_opportunistic_variable_reuse&#39;</span><span class="p">,)</span>

<span class="n">_get_opportunistic_variable_reuse</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span>
    <span class="n">_OPPORTUNISTIC_VARIABLE_REUSE_KEY</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">])</span>

<span class="n">_VARIABLE_RENAME_RULES_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_variable_rename_rules&#39;</span><span class="p">,)</span>

<span class="n">_get_rename_rules_stack</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_VARIABLE_RENAME_RULES_KEY</span><span class="p">,</span>
                                            <span class="k">lambda</span><span class="p">:</span> <span class="p">[])</span>


<div class="viewcode-block" id="OpportunisticVariableReuseScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OpportunisticVariableReuseScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">OpportunisticVariableReuseScope</span><span class="p">(</span><span class="n">enable_opportunistic_reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="n">opportunistic_var_reuse</span> <span class="o">=</span> <span class="n">_get_opportunistic_variable_reuse</span><span class="p">()</span>
  <span class="n">old_val</span> <span class="o">=</span> <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">enable_opportunistic_reuse</span>
  <span class="k">yield</span>
  <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_val</span></div>


<div class="viewcode-block" id="GetOpportunisticVariableReuse"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetOpportunisticVariableReuse">[docs]</a><span class="k">def</span> <span class="nf">GetOpportunisticVariableReuse</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Get the current variable reuse setting.&quot;&quot;&quot;</span>
  <span class="n">opportunistic_var_reuse</span> <span class="o">=</span> <span class="n">_get_opportunistic_variable_reuse</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">opportunistic_var_reuse</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="VariableRenameScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariableRenameScope">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableRenameScope</span><span class="p">(</span><span class="n">renames</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Append the renaming rules to the stack of renames.</span>

<span class="sd">  Args:</span>
<span class="sd">    renames: pairs of (regexp, new_name_format). If the regexp matches, the</span>
<span class="sd">      new_name_format will be interpolated using the matched groups.</span>

<span class="sd">  Yields:</span>
<span class="sd">    scope in which the renaming rules are applied</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">rename_rules_stack</span> <span class="o">=</span> <span class="n">_get_rename_rules_stack</span><span class="p">()</span>
  <span class="n">rename_rules_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">renames</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">rename_rules_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetVariableName"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetVariableName">[docs]</a><span class="k">def</span> <span class="nf">GetVariableName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get variable name after application of all renaming rules.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: untransformed variable name with scope_name prepended</span>

<span class="sd">  Returns:</span>
<span class="sd">    name possibly modified using renaming rules</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">matched</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">new_name</span> <span class="o">=</span> <span class="n">name</span>
  <span class="k">for</span> <span class="n">renames</span> <span class="ow">in</span> <span class="n">_get_rename_rules_stack</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">renames</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">matched</span><span class="p">:</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Multiple matches for: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="n">matched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">new_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">new_name</span> <span class="o">!=</span> <span class="n">name</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;WARNING!!! Renaming variable &#39;</span><span class="si">%s</span><span class="s2">&#39; to &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">new_name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_name</span></div>


<div class="viewcode-block" id="GenerateSeedFromName"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GenerateSeedFromName">[docs]</a><span class="k">def</span> <span class="nf">GenerateSeedFromName</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generate a random seed from a name string.&quot;&quot;&quot;</span>
  <span class="n">md5</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">()</span>
  <span class="n">md5</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">ensure_binary</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
  <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">md5</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="c1"># To keep track of all the variables ever gets created by the CreateVariable</span>
<span class="c1"># routine below.</span>
<span class="n">_ALL_VARS_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__lingvo_all_vars&#39;</span><span class="p">,)</span>

<span class="n">_get_all_vars</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_ALL_VARS_KEY</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>

<span class="n">_VARIABLE_SHAPE_PREFIXES</span> <span class="o">=</span> <span class="n">_ThreadLocalStack</span><span class="p">()</span><span class="o">.</span><span class="n">stack</span>


<div class="viewcode-block" id="VariableShapePrefixContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariableShapePrefixContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">VariableShapePrefixContext</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add a shape prefix to variable created by CreateVariable().</span>

<span class="sd">  Args:</span>
<span class="sd">    shape_prefix: a positive integer of shape prefix.</span>

<span class="sd">  Yields:</span>
<span class="sd">    None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">shape_prefix</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">shape_prefix</span><span class="p">)</span>
  <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape_prefix</span><span class="p">)</span>
  <span class="k">yield</span>
  <span class="n">_VARIABLE_SHAPE_PREFIXES</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetVariableShapePrefixes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetVariableShapePrefixes">[docs]</a><span class="k">def</span> <span class="nf">GetVariableShapePrefixes</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the list of shape prefixes for CreateVariable().&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">_VARIABLE_SHAPE_PREFIXES</span></div>


<div class="viewcode-block" id="GetFanInFanOut"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetFanInFanOut">[docs]</a><span class="k">def</span> <span class="nf">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns (fan_in, fan_out) of a weight variable of the give shape.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Following _compute_fans() from TF&#39;s init_ops.py.</span>
    <span class="k">return</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">receptive_field_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
      <span class="n">receptive_field_size</span> <span class="o">*=</span> <span class="n">s</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="n">fan_out</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">receptive_field_size</span>
    <span class="k">return</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span></div>


<span class="c1"># TODO(yonghui): Add support for partitioned Variables.</span>
<div class="viewcode-block" id="CreateVariable"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CreateVariable">[docs]</a><span class="k">def</span> <span class="nf">CreateVariable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span>
                   <span class="n">params</span><span class="p">,</span>
                   <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">init_wrapper</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">default_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                   <span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates tf.Variable according to param_config.</span>

<span class="sd">  Args:</span>
<span class="sd">    name: A string, name of the variable.</span>
<span class="sd">    params: A WeightParams specifying the details of how this variable should be</span>
<span class="sd">      constructed and initialized.</span>
<span class="sd">    reuse: Whether or not to reuse an existing variable. It has the same</span>
<span class="sd">      semantics as the reuse arg in tf.variable_scope.</span>
<span class="sd">    trainable: Whether or not the variable is trainable.</span>
<span class="sd">    init_wrapper: a callback which takes a tf initializer callable and returns a</span>
<span class="sd">      tensor. It is used when shape of the variable isn&#39;t statically</span>
<span class="sd">      determinable.</span>
<span class="sd">    collections: Override the default variable collection (</span>
<span class="sd">      tf.GraphKeys.GLOBAL_VARIABLES).</span>
<span class="sd">    default_seed: Seed to use for initialization if not specified in params.</span>
<span class="sd">      Used for deterministic initialization in tests.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      tf.VariableSynchronization. By default the synchronization is set to AUTO</span>
<span class="sd">      and the current DistributionStrategy chooses when to synchronize.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class tf.VariableAggregation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    tf.identity(var), var pair. The tf.identity() node is colocated</span>
<span class="sd">    with var. In the case of FLAGS.no_identity_on_vars, simply returns</span>
<span class="sd">    a var, var pair.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">ToStaticShape</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">dim0</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">dim_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]),</span> <span class="n">shape</span>
    <span class="n">dim0</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">method</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">method</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">scale</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">seed</span>

  <span class="k">if</span> <span class="n">IsDefaultParamInit</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">init</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
        <span class="s1">&#39;WARNING!!! var </span><span class="si">%s</span><span class="s1"> is using the default xavier initializer.&#39;</span>
        <span class="s1">&#39; Make sure this is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># We are in a program/test which need determistic randomization.</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">default_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">default_seed</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We are not given a per-variable random seed. We use hash of</span>
        <span class="c1"># variable name as a stable random seed.</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
          <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">var_name</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span>
  <span class="p">]):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="c1"># This is probably not the right method to use when len(shape) &gt; 2,</span>
      <span class="c1"># e.g. dim0 will be 3 with a 3x3 conv2d kernel.</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
          <span class="s1">&#39;Initializing </span><span class="si">%s</span><span class="s1"> of shape </span><span class="si">%s</span><span class="s1"> with method </span><span class="si">%s</span><span class="s1">: dim0=</span><span class="si">%s</span><span class="s1">. &#39;</span>
          <span class="s1">&#39;Make sure that it is intended.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">dim0</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dim0</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">]:</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gaussian_sqrt_fanout&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span><span class="p">]:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fan_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span>

  <span class="n">init_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">real_dtype</span>
  <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_dim&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_sqrt_fanin&#39;</span><span class="p">,</span>
      <span class="s1">&#39;gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform_sqrt_dim&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">scale</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_positive&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform_unit_scaling&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">uniform_unit_scaling_initializer</span><span class="p">(</span>
        <span class="n">factor</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;truncated_gaussian&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_dim&#39;</span><span class="p">,</span>
      <span class="s1">&#39;truncated_gaussian_sqrt_fanin&#39;</span><span class="p">,</span> <span class="s1">&#39;truncated_gaussian_sqrt_fanout&#39;</span>
  <span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">]:</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xavier&#39;</span><span class="p">,</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">]:</span>
    <span class="c1"># pylint: disable=unused-argument</span>
    <span class="k">def</span> <span class="nf">XavierUniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Xavier initialization (x = sqrt(6. / (in + out)); scale*[-x, x]).&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">shape</span><span class="se">\&#39;</span><span class="s1"> must not be </span><span class="se">\&#39;</span><span class="s1">None</span><span class="se">\&#39;</span><span class="s1"> or 0 for XavierUniform&#39;</span><span class="p">)</span>
      <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span> <span class="o">=</span> <span class="n">GetFanInFanOut</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">fan_in</span> <span class="o">+</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;geo_mean_xavier&#39;</span><span class="p">:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span> <span class="o">*</span> <span class="n">fan_out</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

    <span class="c1"># pylint: enable=unused-argument</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">XavierUniform</span>
  <span class="k">elif</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="s1">&#39;kaiming_uniform_fanin_relu&#39;</span><span class="p">,</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span>
  <span class="p">]:</span>
    <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kaiming_uniform_fanin_leakyrelu&#39;</span><span class="p">:</span>
      <span class="c1"># Assume the &#39;a&#39; parameter is the &#39;scale&#39; argument.</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">std_dev</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">std_dev</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
        <span class="n">minval</span><span class="o">=-</span><span class="n">bound</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">bound</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">init_dtype</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;init_type not supported.&#39;</span>
  <span class="k">if</span> <span class="n">init_wrapper</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;Expecting </span><span class="se">\&#39;</span><span class="s1">params.shape</span><span class="se">\&#39;</span><span class="s1"> being None when &#39;</span>
        <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">init_wrapper</span><span class="se">\&#39;</span><span class="s1"> is specified, instead getting </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># Later variable will init from Tensor value instead of intializer callable.</span>
    <span class="n">v_init</span> <span class="o">=</span> <span class="n">init_wrapper</span><span class="p">(</span><span class="n">init_dtype</span><span class="p">,</span> <span class="n">v_init</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">ComplexWrapper</span><span class="p">(</span><span class="n">init</span><span class="p">):</span>

      <span class="k">def</span> <span class="nf">_Wrapper</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">):</span>
        <span class="c1"># A more complex alternative may be to use the init function for</span>
        <span class="c1"># magnitudes and uniform random for phases instead.</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init_dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

      <span class="k">return</span> <span class="n">_Wrapper</span>

    <span class="n">v_init</span> <span class="o">=</span> <span class="n">ComplexWrapper</span><span class="p">(</span><span class="n">v_init</span><span class="p">)</span>

  <span class="c1"># TODO(yonghui): Possibly get away from variable_scope and implement our own</span>
  <span class="c1"># variable sharing mechanism.</span>
  <span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;reuse: Whether to reuse the variables.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">var_shape</span> <span class="o">=</span> <span class="n">GetVariableShapePrefixes</span><span class="p">()</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">var_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="n">var_name</span> <span class="o">=</span> <span class="n">GetVariableName</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">var_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">VariableScope</span><span class="p">(</span>
          <span class="n">scope</span><span class="o">.</span><span class="n">reuse</span><span class="p">,</span>
          <span class="n">custom_getter</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">custom_getter</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">scope</span><span class="o">.</span><span class="n">use_resource</span> <span class="ow">or</span> <span class="n">use_resource_variables</span><span class="p">())</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_scope</span><span class="p">),</span> \
        <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;pin_vars_to_cpu&#39;</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;/cpu:0&#39;</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
              <span class="s1">&#39;var&#39;</span><span class="p">,</span>
              <span class="n">var_shape</span><span class="p">,</span>
              <span class="n">dtype</span><span class="p">,</span>
              <span class="n">v_init</span><span class="p">,</span>
              <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
              <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
              <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">var_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
              <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
              <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s1">&#39;var&#39;</span><span class="p">,</span>
            <span class="n">var_shape</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">,</span>
            <span class="n">v_init</span><span class="p">,</span>
            <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">var_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">_get_opportunistic_variable_reuse</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>  <span class="c1"># Possibly the variable already exists</span>
      <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">(</span><span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">GetVar</span><span class="p">()</span>

  <span class="n">var_ref</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">experimental_ref</span><span class="p">()</span>  <span class="c1"># For key in dict/set.</span>
  <span class="n">all_vars</span> <span class="o">=</span> <span class="n">_get_all_vars</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">var_ref</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Reusing var </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">cached</span> <span class="o">=</span> <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">cached</span> <span class="o">==</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;Cached config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> vs new config:</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                         <span class="p">(</span><span class="n">cached</span><span class="o">.</span><span class="n">ToText</span><span class="p">(),</span> <span class="n">p</span><span class="o">.</span><span class="n">ToText</span><span class="p">()))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Creating var </span><span class="si">%s</span><span class="s1"> shape=</span><span class="si">%s</span><span class="s1"> on device </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">all_vars</span><span class="p">[</span><span class="n">var_ref</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">Copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">collections</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;no_identity_on_vars&#39;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">var</span><span class="p">,</span> <span class="n">var</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># This tf.identity colocated with var.</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">var</span><span class="p">),</span> <span class="n">var</span></div>


<span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="GetGlobalVariableScope"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetGlobalVariableScope">[docs]</a><span class="k">def</span> <span class="nf">GetGlobalVariableScope</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets the global variable scope (as if no variable_scope has been set).</span>

<span class="sd">  Returns:</span>
<span class="sd">    The VariableScope corresponding to as if no tf.variable_scope is in effect.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">_global_variable_scope</span><span class="p">:</span>
    <span class="c1"># Each thread gets its own default global variable scope, and we take</span>
    <span class="c1"># advantage of that in order to get a top-level scope. This avoids the</span>
    <span class="c1"># need to call tf.get_variable_scope() at the module level, which allows</span>
    <span class="c1"># this module to be imported without modifying global state (i.e. creating</span>
    <span class="c1"># the default graph). It is important to not mutate the global state at</span>
    <span class="c1"># module load time, because it let&#39;s us flip flags after import that affect</span>
    <span class="c1"># core TensorFlow behavior.</span>
    <span class="k">def</span> <span class="nf">Initialize</span><span class="p">():</span>
      <span class="k">global</span> <span class="n">_global_variable_scope</span>
      <span class="n">_global_variable_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>

    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">Initialize</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">_global_variable_scope</span></div>


<span class="n">_GLOBAL_STEP_STACK</span> <span class="o">=</span> <span class="p">[]</span>


<div class="viewcode-block" id="GlobalStepContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GlobalStepContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">GlobalStepContext</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">):</span>
  <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">yield</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="k">raise</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">_GLOBAL_STEP_STACK</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetGlobalStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetGlobalStep">[docs]</a><span class="k">def</span> <span class="nf">GetGlobalStep</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_GLOBAL_STEP_STACK</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">()</span></div>


<div class="viewcode-block" id="GetOrCreateGlobalStepVar"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetOrCreateGlobalStepVar">[docs]</a><span class="k">def</span> <span class="nf">GetOrCreateGlobalStepVar</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Return the global_step variable, creating it if it does not exist.</span>

<span class="sd">  Prefer GetGlobalStep if a tensor rather than a tf.Variable is sufficient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The global_step variable, or a new created one if it does not exist.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
      <span class="n">GetGlobalVariableScope</span><span class="p">(),</span> <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource_variables</span><span class="p">()):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span></div>


<div class="viewcode-block" id="LogMultiLines"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.LogMultiLines">[docs]</a><span class="k">def</span> <span class="nf">LogMultiLines</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">lines</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span></div>


<div class="viewcode-block" id="_LogPlacement"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._LogPlacement">[docs]</a><span class="k">def</span> <span class="nf">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Logs theta and its copy&#39;s device placement.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">GetDevices</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Flatten a `.NestedMap` m and extracts each value&#39;s device.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()]</span>

  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;=== </span><span class="si">%s</span><span class="s1"> ===&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
  <span class="n">LogMultiLines</span><span class="p">(</span>
      <span class="n">label</span><span class="p">,</span>
      <span class="n">theta</span><span class="o">.</span><span class="n">Pack</span><span class="p">([(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> -&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">GetDevices</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">GetDevices</span><span class="p">(</span><span class="n">copy</span><span class="p">))</span>
                 <span class="p">])</span><span class="o">.</span><span class="n">DebugString</span><span class="p">())</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;==========&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="CreateLocalTheta"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CreateLocalTheta">[docs]</a><span class="k">def</span> <span class="nf">CreateLocalTheta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">device_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates local copy of theta and shards across devices device list.</span>

<span class="sd">  Leaves variables intact.</span>

<span class="sd">  Args:</span>
<span class="sd">    theta: a `.NestedMap` of variables.</span>
<span class="sd">    device_list: list of devices to shard across. If None, defaults to a list</span>
<span class="sd">      [&#39;&#39;].</span>
<span class="sd">    label: Logging label.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of identity() wrapped theta</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">class</span> <span class="nc">AddIdentity</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_list</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_list</span> <span class="o">=</span> <span class="n">device_list</span> <span class="k">if</span> <span class="n">device_list</span> <span class="k">else</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_list</span><span class="p">)]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">copy</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">AddIdentity</span><span class="p">(</span><span class="n">device_list</span><span class="p">))</span>
  <span class="n">_LogPlacement</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">copy</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">copy</span></div>


<div class="viewcode-block" id="_GetVarsToLoad"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetVarsToLoad">[docs]</a><span class="k">def</span> <span class="nf">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Determines variables to load and their names in checkpoint.&quot;&quot;&quot;</span>
  <span class="c1"># This list contains mappings from var names as they appear in the checkpoint</span>
  <span class="c1"># to the vars in our model they correspond to.</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">model_var</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">regexp</span><span class="p">,</span> <span class="n">name_format</span> <span class="ow">in</span> <span class="n">variable_loading_rules</span><span class="p">:</span>
      <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">regexp</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># Skip if var doesn&#39;t match the loading rules, or if it should be ignored.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">match</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span>
          <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">model_var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
        <span class="k">continue</span>
      <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">name_format</span> <span class="o">%</span> <span class="n">match</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">checkpoint_var_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;:0&#39;</span><span class="p">):</span>
        <span class="n">checkpoint_var_name</span> <span class="o">=</span> <span class="n">checkpoint_var_name</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Loading </span><span class="si">%s</span><span class="s1"> from </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">model_var</span><span class="p">,</span> <span class="n">checkpoint_var_name</span><span class="p">)</span>
      <span class="n">vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">checkpoint_var_name</span><span class="p">,</span> <span class="n">model_var</span><span class="p">))</span>
      <span class="k">break</span>
  <span class="k">return</span> <span class="n">vars_to_load</span></div>


<div class="viewcode-block" id="OverrideVarsFromCheckpoint"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OverrideVarsFromCheckpoint">[docs]</a><span class="k">def</span> <span class="nf">OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span>
                               <span class="n">variable_loading_rules</span><span class="p">,</span> <span class="n">var_ignore_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides variables from a provided checkpoint.&quot;&quot;&quot;</span>
  <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">variable_loading_rules</span><span class="p">,</span>
                                <span class="n">var_ignore_rules</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s1">&#39;Variable loading rules did not match any vars. &#39;</span>
                      <span class="s1">&#39;All known: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">)</span> <span class="o">%</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_vars</span><span class="p">])</span>
  <span class="n">load_var_names</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">])</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from checkpoint: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">load_var_names</span><span class="p">)</span>

  <span class="k">while</span> <span class="n">vars_to_load</span><span class="p">:</span>
    <span class="c1"># When restoring, it&#39;s possible the same value in the checkpoint</span>
    <span class="c1"># can be restored to multiple variables (e.g. during</span>
    <span class="c1"># distillation).  However, tf.train.Saver, since it&#39;s used for</span>
    <span class="c1"># both saving and restoring, requires the name in the checkpoint</span>
    <span class="c1"># to be unique for each variable.  So, we call it multiple times</span>
    <span class="c1"># with a unique set of names each time.</span>
    <span class="n">unique_vars_to_load</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">remaining_vars_to_load</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vars_to_load</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unique_vars_to_load</span><span class="p">:</span>
        <span class="n">unique_vars_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">remaining_vars_to_load</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="n">unique_vars_to_load</span><span class="p">)</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">vars_to_load</span> <span class="o">=</span> <span class="n">remaining_vars_to_load</span></div>


<div class="viewcode-block" id="OverrideVarsFromCheckpoints"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.OverrideVarsFromCheckpoints">[docs]</a><span class="k">def</span> <span class="nf">OverrideVarsFromCheckpoints</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpts_loading_rules</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Overrides model variables from checkpoints.</span>

<span class="sd">  Args:</span>
<span class="sd">    session: Tensorflow session.</span>
<span class="sd">    all_vars: List of all the parameters in the model.</span>
<span class="sd">    ckpts_loading_rules: A dictionary of checkpoint path: loading rules.</span>
<span class="sd">      Checkpoint path must be a path to a pretrained model, and loading rules is</span>
<span class="sd">      expected to be a tuple of two lists. The first consisting of tuples of</span>
<span class="sd">      strings defining (regex to match parameter names in the model to override,</span>
<span class="sd">      format string to determine the corresponding var in the checkpoint), and</span>
<span class="sd">      the second list consisting of a list of regexes to match parameter names</span>
<span class="sd">      in the model which should not be overridden, even if they match those in</span>
<span class="sd">      the loading rules.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if colliding vars exist or loading rules is not a list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ckpts_loading_rules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from multiple checkpoints.&#39;</span><span class="p">)</span>

  <span class="n">var_refs_overridden</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span> <span class="ow">in</span> <span class="n">ckpts_loading_rules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Overriding vars from checkpoint: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loading_rules</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">loading_rules</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Loading rules for </span><span class="si">%s</span><span class="s1"> must be a tuple of two lists!&#39;</span> <span class="o">%</span>
                       <span class="n">ckpt_path</span><span class="p">)</span>

    <span class="c1"># Filter the model variables to be overridden.</span>
    <span class="n">var_refs_to_override</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">var</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">experimental_ref</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">_GetVarsToLoad</span><span class="p">(</span><span class="n">all_vars</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">]</span>

    <span class="n">overlap_refs</span> <span class="o">=</span> <span class="nb">set</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">var_refs_overridden</span><span class="p">,</span> <span class="n">var_refs_to_override</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">overlap_refs</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Colliding variables to override: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">overlap_refs</span><span class="p">)</span>

    <span class="n">OverrideVarsFromCheckpoint</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">,</span> <span class="n">loading_rules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                               <span class="n">loading_rules</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">var_refs_overridden</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">var_refs_to_override</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Model variables overridden: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">var_refs_overridden</span><span class="p">)</span></div>


<div class="viewcode-block" id="ComputeGradientsSimple"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeGradientsSimple">[docs]</a><span class="k">def</span> <span class="nf">ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                           <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span>
      <span class="n">loss</span><span class="p">,</span>
      <span class="n">all_vars</span><span class="p">,</span>
      <span class="n">aggregation_method</span><span class="o">=</span><span class="n">grad_aggregation_method</span><span class="p">,</span>
      <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
      <span class="n">gate_gradients</span><span class="o">=</span><span class="n">gate_gradients</span><span class="p">)</span></div>


<div class="viewcode-block" id="ComputeTpuEmbeddingGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeTpuEmbeddingGradients">[docs]</a><span class="k">def</span> <span class="nf">ComputeTpuEmbeddingGradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">activation_dict</span><span class="p">,</span> <span class="n">tpu_embedding</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a TpuEmbedding SendGradient op.</span>

<span class="sd">  Args:</span>
<span class="sd">   loss: The loss to backprop from.</span>
<span class="sd">   activation_dict: String feature -&gt; embedding activations dict.</span>
<span class="sd">   tpu_embedding: TPUEmbedding instance.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Scale the loss to account for the full batch size.</span>
  <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
  <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
  <span class="n">feature_to_gradient_dict</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span>
      <span class="nb">zip</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">activation_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="n">grads</span><span class="p">))</span>
  <span class="n">send_gradient_op</span> <span class="o">=</span> <span class="n">tpu_embedding</span><span class="o">.</span><span class="n">generate_send_gradients_op</span><span class="p">(</span>
      <span class="n">feature_to_gradient_dict</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">send_gradient_op</span></div>


<div class="viewcode-block" id="_ComputeGradientsTpu"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._ComputeGradientsTpu">[docs]</a><span class="k">def</span> <span class="nf">_ComputeGradientsTpu</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span>
                         <span class="n">all_vars</span><span class="p">,</span>
                         <span class="n">grad_aggregation_method</span><span class="p">,</span>
                         <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                         <span class="n">gate_gradients</span><span class="p">,</span>
                         <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients for local loss across whole TPU cluster.</span>

<span class="sd">  This implementation specializes for the case where weight params maybe used</span>
<span class="sd">  for different number of times in the forward computation, so that gradients</span>
<span class="sd">  should be normalized by the actual number of times they are being computed.</span>

<span class="sd">  TODO(yonghui): Maybe merge this implementation with the _ComputeGradientsTpu</span>
<span class="sd">  one.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: The loss to backprop from.</span>
<span class="sd">    all_vars: Vars with respect to which gradients are to be computed.</span>
<span class="sd">    grad_aggregation_method: aggregation method to use when calling</span>
<span class="sd">      tf.gradients.</span>
<span class="sd">    colocate_gradients_with_ops: boolean, whether or not to colocate gradient op</span>
<span class="sd">      with the original op.</span>
<span class="sd">    gate_gradients: boolean, flag to be passed to tf.gradients.</span>
<span class="sd">    skip_zero_gradients: whether to skip zero gradients during aggregation.</span>
<span class="sd">    use_bf16_gradients_ar: Whether to use bfloat16 dtype for gradients</span>
<span class="sd">      all-reduce.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Gradients to be passed back.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: upon invalid arguments.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_zero_gradients</span><span class="p">:</span>
    <span class="c1"># Scale the loss to account for the full batch size.</span>
    <span class="n">shards</span> <span class="o">=</span> <span class="n">tpu_function</span><span class="o">.</span><span class="n">get_tpu_context</span><span class="p">()</span><span class="o">.</span><span class="n">number_of_shards</span>
    <span class="k">assert</span> <span class="n">shards</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="c1"># Computes the gradients.</span>
  <span class="c1"># Sum the grads so that we can compute statistics across the whole batch.</span>
  <span class="n">all_grads</span> <span class="o">=</span> <span class="n">ComputeGradientsSimple</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">all_vars</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                                     <span class="n">colocate_gradients_with_ops</span><span class="p">,</span>
                                     <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># NOTE: We can&#39;t use tpu_optimizer.CrossShardOptimizer since</span>
  <span class="c1"># we need to scale the grads *after* the cross_replica_sum to</span>
  <span class="c1"># match GPU version!</span>

  <span class="c1"># TODO(cwhipkey): should we do something different here? - we could do</span>
  <span class="c1"># some operations on the gradients before the aggregation (see comments in</span>
  <span class="c1"># tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py - see compute_gradients -</span>
  <span class="c1"># for some more details).</span>

  <span class="n">aggregated_grads</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">all_grads</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">continue</span>
    <span class="k">if</span> <span class="n">use_bf16_gradients_ar</span><span class="p">:</span>
      <span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">g</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">skip_zero_gradients</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># loss is already scaled by 1/shards.</span>
        <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Compute the cross-replica mean of &#39;g&#39;, skipping zero gradients.</span>

        <span class="c1"># Q(yonghui): Is there a better way to detect a non-zero gradient?</span>
        <span class="c1"># Note(yonghui): gradient of a weight can be zero if that</span>
        <span class="c1"># weight is not used in the forward computation, e.g. as in</span>
        <span class="c1"># switchable layers in neural architecture search, pruned by channel</span>
        <span class="c1"># mask, or sparsified.</span>
        <span class="k">if</span> <span class="n">skip_zero_gradients</span> <span class="o">==</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span>
          <span class="c1"># Same shape as &#39;g&#39;.</span>
          <span class="n">g_is_non_zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">skip_zero_gradients</span> <span class="o">==</span> <span class="s1">&#39;variable&#39;</span><span class="p">:</span>
          <span class="c1"># A variable-wide 0/1 scalar.</span>
          <span class="n">g_is_non_zero</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">g</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mf">1e-24</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown skip_zero_gradients: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                           <span class="n">skip_zero_gradients</span><span class="p">)</span>
        <span class="n">num_updates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g_is_non_zero</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">normalized_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tpu</span><span class="o">.</span><span class="n">cross_replica_sum</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_updates</span>
      <span class="n">aggregated_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">normalized_g</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">aggregated_grads</span></div>


<div class="viewcode-block" id="VarGrad"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VarGrad">[docs]</a><span class="k">class</span> <span class="nc">VarGrad</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A class that holds a variable and a gradient.&quot;&quot;&quot;</span>

  <span class="n">_VAR_GRAD</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;VarGradNamedTuple&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">])</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_VAR_GRAD</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;VarGrad(</span><span class="si">%r</span><span class="s1">, </span><span class="si">%r</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_var_grad</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span></div>


<div class="viewcode-block" id="ComputeGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ComputeGradients">[docs]</a><span class="k">def</span> <span class="nf">ComputeGradients</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">,</span>
    <span class="n">vmap</span><span class="p">,</span>
    <span class="n">grad_aggregation_method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">AggregationMethod</span><span class="o">.</span><span class="n">EXPERIMENTAL_TREE</span><span class="p">,</span>
    <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">compute_gradients_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: A scalar Tensor.</span>
<span class="sd">    vmap: A `.NestedMap` of variables.</span>
<span class="sd">    grad_aggregation_method: Specifies the method used to combine gradient</span>
<span class="sd">      terms. Accepted values are constants defined in the class</span>
<span class="sd">      AggregationMethod.</span>
<span class="sd">    colocate_gradients_with_ops: If True, try colocating gradients with the</span>
<span class="sd">      corresponding op.</span>
<span class="sd">    gate_gradients: If True, add a tuple around the gradients returned for an</span>
<span class="sd">      operations. This avoids some race conditions.</span>
<span class="sd">    compute_gradients_fn: Function to use to compute gradients. If None, use</span>
<span class="sd">      default. compute_gradients_fn should have the same signature as this</span>
<span class="sd">      function, but without the last argument.</span>
<span class="sd">    skip_zero_gradients: Whether to skip aggregating zero gradients. This helps</span>
<span class="sd">      in case where some weights may not be used in forward computation, e.g.,</span>
<span class="sd">      sparsely activated networks or switchable layers in neural architectural</span>
<span class="sd">      search. Only applicable on TPU.</span>
<span class="sd">      Possible values are:</span>

<span class="sd">        * None: do not skip zero gradients;</span>
<span class="sd">        * `variable`: skip if the entire variable&#39;s gradients are almost zero;</span>
<span class="sd">          reduce_sum(abs(grads)) &lt; 1e-8.</span>
<span class="sd">        * `weight`: skip if the individual weight&#39;s gradients are almost zero:</span>
<span class="sd">          abs(grad) &lt; 1e-8.</span>
<span class="sd">    use_bf16_gradients_ar: Whether to use bfloat16 dtype for gradients</span>
<span class="sd">      all-reduce. This applies to TPU only.</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of VarGrad. You can view</span>
<span class="sd">    var_grad as an ordered list of (key, (var, grad)) tuples. Every</span>
<span class="sd">    key of var_grad exists in vmap. Every variable in vmap that</span>
<span class="sd">    contributes to loss must exist in var_grad. Every var of var_grad</span>
<span class="sd">    must exist in vmap.  grad is the corresponding gradient computed</span>
<span class="sd">    for var. grad is guaranteed to be not None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vmap</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">skip_zero_gradients</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>

  <span class="c1"># Uniqify and remove None.</span>
  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">_Unique</span><span class="p">())</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="c1"># Filter out variables not contributing to &#39;loss&#39;.</span>
  <span class="n">trainable_variables</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">())</span>
  <span class="n">dependent_ops_and_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">FindNeeded</span><span class="p">([</span><span class="n">loss</span><span class="p">]))</span>

  <span class="k">def</span> <span class="nf">Needed</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trainable_variables</span><span class="p">:</span>
        <span class="c1"># Skip non-trainable variables. Otherwise,</span>
        <span class="c1"># tf.Optimizer.apply_gradients throws up an exception instead</span>
        <span class="c1"># of skipping the update.</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="n">filtered_vmap</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">Needed</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">filtered_vmap</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">filtered_vlist</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

  <span class="c1"># Use caller-supplied gradient function if supplied.</span>
  <span class="k">if</span> <span class="n">compute_gradients_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">take_grad</span> <span class="o">=</span> <span class="n">compute_gradients_fn</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># tpu vs non-tpu is slightly different.</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
          <span class="n">_ComputeGradientsTpu</span><span class="p">,</span>
          <span class="n">skip_zero_gradients</span><span class="o">=</span><span class="n">skip_zero_gradients</span><span class="p">,</span>
          <span class="n">use_bf16_gradients_ar</span><span class="o">=</span><span class="n">use_bf16_gradients_ar</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">take_grad</span> <span class="o">=</span> <span class="n">ComputeGradientsSimple</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">take_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">filtered_vlist</span><span class="p">,</span> <span class="n">grad_aggregation_method</span><span class="p">,</span>
                    <span class="n">colocate_gradients_with_ops</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="p">)</span>

  <span class="c1"># Formulate pairs of (var, grad) and pack them into the same</span>
  <span class="c1"># structure as filtered_vmap.</span>
  <span class="n">var_grads</span> <span class="o">=</span> <span class="n">filtered_vmap</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span>
      <span class="p">[</span><span class="n">VarGrad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">filtered_vlist</span><span class="p">,</span> <span class="n">grads</span><span class="p">)])</span>

  <span class="c1"># TPU training is not compatible with the variable name check below when</span>
  <span class="c1"># control flow v2 is enabled. The main reason is the body function will be</span>
  <span class="c1"># encapsulated as a TF function while variables will be lifted out, and as a</span>
  <span class="c1"># result dependent_ops_and_tensors will not contain any variables. See</span>
  <span class="c1"># b/150689507 for more info.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">control_flow_v2_enabled</span><span class="p">():</span>
    <span class="c1"># Check that gradients for variables that are not needed by current task is</span>
    <span class="c1"># empty.</span>
    <span class="k">def</span> <span class="nf">CheckGrad</span><span class="p">(</span><span class="n">vg</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">vg</span><span class="o">.</span><span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dependent_ops_and_tensors</span> <span class="ow">and</span> <span class="n">vg</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;Variable </span><span class="si">%s</span><span class="s1"> is not a dependent of </span><span class="si">%s</span><span class="s1">, expect &#39;</span>
                   <span class="s1">&#39;gradient be None, but got </span><span class="si">%s</span><span class="s1">. This should not happen, &#39;</span>
                   <span class="s1">&#39;please contact the owner of b/150689507 for further &#39;</span>
                   <span class="s1">&#39;investigation.&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">vg</span><span class="o">.</span><span class="n">var</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">vg</span><span class="o">.</span><span class="n">grad</span><span class="p">)))</span>
        <span class="k">assert</span> <span class="kc">False</span><span class="p">,</span> <span class="n">err_msg</span>
      <span class="k">return</span> <span class="kc">True</span>

    <span class="n">var_grads</span> <span class="o">=</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="n">CheckGrad</span><span class="p">)</span>

  <span class="c1"># Removes pairs whose grad is None.</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">FlattenItems</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;ComputeGradients drops </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">var_grad</span><span class="p">:</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaskGradients"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MaskGradients">[docs]</a><span class="k">def</span> <span class="nf">MaskGradients</span><span class="p">(</span><span class="n">var_grad</span><span class="p">,</span> <span class="n">grad_mask</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes gradients of non-masked variables in vmap w.r.t loss.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grad: A `.NestedMap` of (variable, gradient)</span>
<span class="sd">    grad_mask: A dict of (variable name, mask).</span>

<span class="sd">  Returns:</span>
<span class="sd">    var_grad - a `.NestedMap` of (variable, mask * gradient).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">ApplyMask</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">entry</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">grad_mask</span><span class="p">[</span><span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">var_grad</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">ApplyMask</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyGradMultiplier"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyGradMultiplier">[docs]</a><span class="k">def</span> <span class="nf">ApplyGradMultiplier</span><span class="p">(</span><span class="n">vs_gs</span><span class="p">,</span> <span class="n">grad_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Scale gradients by grad_scale on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs: A `.NestedMap` of VarGrad.</span>
<span class="sd">    grad_scale: If None, each vs_gs entry has the scale. Otherwise, grad_scale</span>
<span class="sd">      applies to every entry.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of (variable, gradient * grad_scale). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="mf">0.</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Scale</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad_scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">scale</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">scale</span> <span class="o">=</span> <span class="n">grad_scale</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ScaleOrZero</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="HasNanOrInfGradient"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.HasNanOrInfGradient">[docs]</a><span class="k">def</span> <span class="nf">HasNanOrInfGradient</span><span class="p">(</span><span class="n">var_grads</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a bool tensor to indicate if `var_grads` contains NaNs or Infs.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: A `.NestedMap` with (var, grad) tuple as the map value.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A bool scalar tensor to indicate if the `var_grads` contains NaNs or Infs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">HasNanOrInf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">values</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">([</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">x</span><span class="p">))])</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">is_nan</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">is_inf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_any</span><span class="p">([</span><span class="n">HasNanOrInf</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="n">var_grads</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()])</span></div>


<div class="viewcode-block" id="ApplyGradNormClipping"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyGradNormClipping">[docs]</a><span class="k">def</span> <span class="nf">ApplyGradNormClipping</span><span class="p">(</span><span class="n">vs_gs</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Clip gradients to norm on same device as corresponding variables.</span>

<span class="sd">  Args:</span>
<span class="sd">    vs_gs: A `.NestedMap` of VarGrad.</span>
<span class="sd">    norm: Each tensor&#39;s gradient will be scaled down to have a maximum L2-norm</span>
<span class="sd">      value of `norm`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` of VarGrad(variable, scaled_gradient). In particular, if</span>
<span class="sd">    grad_scale is 0, the result gradient is always 0, even if the input</span>
<span class="sd">    gradient is inf or nan.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">CheckNumerics</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="s1">&#39;Gradient for </span><span class="si">%s</span><span class="s1"> is not finite.&#39;</span> <span class="o">%</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Clip</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Scales the gradient.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">assert</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;No grad found for &#39;</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span>
            <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">norm</span><span class="p">),</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">ClipByNorm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vs_gs</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">Clip</span><span class="p">)</span></div>


<span class="n">SKIP_LP_REGULARIZATION</span> <span class="o">=</span> <span class="s1">&#39;__lingvo_skip_lp_regularization&#39;</span>


<div class="viewcode-block" id="AdjustGradientsWithLpLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AdjustGradientsWithLpLoss">[docs]</a><span class="k">def</span> <span class="nf">AdjustGradientsWithLpLoss</span><span class="p">(</span><span class="n">var_grads</span><span class="p">,</span> <span class="n">lp_regularizer_weight</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adjusts the map of (var, grad) with Lp regularization, where p=1.0 or 2.0.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_grads: a `.NestedMap` or list of (variable, gradient).</span>
<span class="sd">    lp_regularizer_weight: Lp regularization weight.</span>
<span class="sd">    p: For now we support 1.0 or 2.0.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (lp_loss, var_grads).</span>

<span class="sd">    - lp_loss: A scalar. The lp loss.</span>
<span class="sd">    - var_grads: a `.NestedMap` or list of (variable, gradient) regulated by Lp.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(yuancao): For now we support p=1 or 2, but this can be extended to</span>
  <span class="c1"># lp-norm in general.</span>

  <span class="k">assert</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="s1">&#39;For now we only support L1/L2 regularization.&#39;</span>

  <span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">uniq_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="o">.</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">uniq_ids</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">var</span>

  <span class="k">def</span> <span class="nf">ShouldAdjust</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">)</span>

  <span class="n">filtered_var_grads</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">var_grad</span> <span class="k">for</span> <span class="n">var_grad</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">var_grads</span><span class="p">)</span> <span class="k">if</span> <span class="n">ShouldAdjust</span><span class="p">(</span><span class="n">var_grad</span><span class="o">.</span><span class="n">var</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="n">filtered_vars</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">GetVar</span><span class="p">,</span> <span class="n">filtered_var_grads</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">filtered_vars</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;AdjustGradientsWithLpLoss: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumSquared</span><span class="p">(</span><span class="n">filtered_vars</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
    <span class="n">lp_loss</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">SumAbs</span><span class="p">(</span><span class="n">filtered_vars</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LpGrad</span><span class="p">(</span><span class="n">var_grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adjusts item&#39;s grad w/ Lp loss term.&quot;&quot;&quot;</span>
    <span class="n">var</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">var_grad</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="c1"># Question(rpang): do we apply Lp loss here even if &#39;var&#39; is in</span>
      <span class="c1"># SKIP_LP_REGULARIZATION?</span>
      <span class="c1">#</span>
      <span class="c1"># Note: IndexedSlces appears for embedding lookups.</span>
      <span class="c1"># Embedding lookup ids can have duplicate. For duplicated ids, we</span>
      <span class="c1"># only want to consider once for each ids.</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">emb</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>  <span class="c1"># [#ids, dims]</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="c1"># Counts is a vector of size vocab_size. counts[i] is i-th words</span>
        <span class="c1"># occurances in &#39;ids&#39;.</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ids</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

        <span class="c1"># Gradients for duplicated ids will be summed when they get</span>
        <span class="c1"># applied, and hence we account for that by first dividing</span>
        <span class="c1"># gradient resulting from lp loss by how many times the id is</span>
        <span class="c1"># duplicated.</span>
        <span class="c1">#</span>
        <span class="c1"># For each id in &#39;ids&#39;, we know counts[id] is non-zero,</span>
        <span class="c1"># hence, it&#39;s always safe to take reciprocal.</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ids</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [#ids, 1]</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">values</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">grad_v</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">ids</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">var</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">SKIP_LP_REGULARIZATION</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">var</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
          <span class="n">grad_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">lp_regularizer_weight</span> <span class="o">*</span> <span class="n">grad_v</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">+=</span> <span class="n">delta</span>
    <span class="k">return</span> <span class="n">VarGrad</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">lp_loss</span><span class="p">,</span> <span class="n">Transform</span><span class="p">(</span><span class="n">LpGrad</span><span class="p">,</span> <span class="n">var_grads</span><span class="p">)</span></div>


<div class="viewcode-block" id="SplitRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SplitRecursively">[docs]</a><span class="k">def</span> <span class="nf">SplitRecursively</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Splits Tensors in &#39;x&#39; recursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: a Tensor, or a list or NestMap containing Tensors to split.</span>
<span class="sd">    num_splits: number of splits per Tensor.</span>
<span class="sd">    axis: the split axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of split values of length &#39;num_splits&#39;.</span>

<span class="sd">    - If &#39;x&#39; is a Tensor, a list of split Tensors.</span>
<span class="sd">    - If &#39;x&#39; is a list, a list of lists, where each sublist has the same length</span>
<span class="sd">      as &#39;x&#39; and the k&#39;th element in each sublist corresponds to a split of the</span>
<span class="sd">      k&#39;th element from &#39;x&#39;.</span>
<span class="sd">    - If &#39;x&#39; is a `.NestedMap`, a list of `.NestedMap`, where each field</span>
<span class="sd">      corresponds to a split from the same field of &#39;x&#39;.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="n">SplitRecursively</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">splits</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">NestedMap</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="n">val_splits</span> <span class="o">=</span> <span class="n">SplitRecursively</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_splits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for SplitRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span></div>


<div class="viewcode-block" id="ConcatRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatRecursively">[docs]</a><span class="k">def</span> <span class="nf">ConcatRecursively</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates tensors from &#39;splits&#39;.</span>

<span class="sd">  This is the inverse function of SplitRecursively.</span>

<span class="sd">  Args:</span>
<span class="sd">    splits: a list of splits to concatenate, where elements can be Tensors,</span>
<span class="sd">      lists, or `.NestedMap`. The elements must share the same type and</span>
<span class="sd">      structure.  For example, list elements must have the same length;</span>
<span class="sd">      `.NestedMap` must have the same set of fields.</span>
<span class="sd">    axis: the concatenation axis.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Concatenated data.</span>

<span class="sd">    - If input &#39;splits&#39; are Tensors, returns a concatenated Tensor.</span>
<span class="sd">    - If input &#39;splits&#39; are lists, returns a list of the same length where the</span>
<span class="sd">      k&#39;th element represents concatenated data of the k&#39;th element from each</span>
<span class="sd">      split.</span>
<span class="sd">    - If input &#39;splits&#39; are `.NestedMap`, returns a `.NestedMap` with each field</span>
<span class="sd">      concatenated from corresponding fields of input splits.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if &#39;splits&#39; is not a list or elements of &#39;splits&#39; do not have</span>
<span class="sd">      known or matching types.</span>
<span class="sd">    ValueError: if &#39;splits&#39; is empty or elements of &#39;splits&#39; do not have</span>
<span class="sd">      matching structures.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Non-list inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">splits</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Empty inputs for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>

  <span class="n">tmpl</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">splits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Length mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                           <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tmpl</span><span class="p">))</span>
    <span class="p">]</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmpl</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">)</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Type mismatch for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">splits</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">tmpl</span><span class="p">:</span>
      <span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ConcatRecursively</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">],</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unexpected type for ConcatRecursively: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span></div>


<div class="viewcode-block" id="AddToPruningCollections"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddToPruningCollections">[docs]</a><span class="k">def</span> <span class="nf">AddToPruningCollections</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span>
                            <span class="n">mask</span><span class="p">,</span>
                            <span class="n">threshold</span><span class="p">,</span>
                            <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">old_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">old_old_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add mask, threshold, and weight vars to their respective collections.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">mask</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">MASK_COLLECTION</span><span class="p">):</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">WEIGHT_COLLECTION</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">MASK_COLLECTION</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">THRESHOLD_COLLECTION</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

    <span class="c1"># Add gradient, old_weight, and old_old_weight to collections approximating</span>
    <span class="c1"># gradient and hessian, where old_weight is the weight tensor one step</span>
    <span class="c1"># before and old_old_weight is the weight tensor two steps before.</span>
    <span class="k">if</span> <span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">old_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="k">assert</span> <span class="n">old_old_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">WEIGHT_GRADIENT_COLLECTION</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">OLD_WEIGHT_COLLECTION</span><span class="p">,</span> <span class="n">old_weight</span><span class="p">)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">pruning</span><span class="o">.</span><span class="n">OLD_OLD_WEIGHT_COLLECTION</span><span class="p">,</span> <span class="n">old_old_weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="WeightedAvg"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightedAvg">[docs]</a><span class="k">def</span> <span class="nf">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">sum_reduction_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes weighted average of values from a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a tensor of values</span>
<span class="sd">    weights: a tensor of weights</span>
<span class="sd">    sum_reduction_fn: called to reduce the values and weights to single value</span>
<span class="sd">    name: name of metric.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple (avg, total_weight).</span>

<span class="sd">    - avg: weighted average value</span>
<span class="sd">    - total_weight: sum of all weights</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;shape of values and weights tensors must match for metric &#39;</span> <span class="o">+</span> <span class="n">name</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">(</span>
      <span class="p">[</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">message</span><span class="o">=</span><span class="n">msg</span><span class="p">)],</span> <span class="n">values</span><span class="p">)</span>
  <span class="n">total_weight</span> <span class="o">=</span> <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
  <span class="n">avg</span> <span class="o">=</span> <span class="n">sum_reduction_fn</span><span class="p">(</span><span class="n">values</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">total_weight</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">avg</span><span class="p">,</span> <span class="n">total_weight</span></div>


<div class="viewcode-block" id="WeightedAvgOfMetrics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WeightedAvgOfMetrics">[docs]</a><span class="k">def</span> <span class="nf">WeightedAvgOfMetrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the weighted average of metrics in the list.</span>

<span class="sd">  Args:</span>
<span class="sd">    metrics: list of dictionaries of metrics</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - dictionary of weighted averages of each metrics.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_metrics</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_metrics</span><span class="p">:</span>
        <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_metrics</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">value</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values_and_weights</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">lists_of_metrics</span><span class="p">)):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values_and_weights</span><span class="p">])</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">WeightedAvg</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span></div>


<div class="viewcode-block" id="ConcatPerExampleTensors"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatPerExampleTensors">[docs]</a><span class="k">def</span> <span class="nf">ConcatPerExampleTensors</span><span class="p">(</span><span class="n">per_example</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenate per-example tensors from many hosts into one large block.</span>

<span class="sd">  Args:</span>
<span class="sd">    per_example: list of dictionaries of per-example tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    ret_dict - string -&gt; concatenated tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ret_dict</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">lists_of_per_example</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">per_example</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lists_of_per_example</span><span class="p">:</span>
        <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lists_of_per_example</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">lists_of_per_example</span><span class="p">)):</span>
    <span class="n">ret_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ret_dict</span></div>


<div class="viewcode-block" id="CombineMetrics"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CombineMetrics">[docs]</a><span class="k">def</span> <span class="nf">CombineMetrics</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Combines metrics from `loss_metric_weight_pairs` according to weights.</span>

<span class="sd">  Keys must either exist in all metrics, in which it will be processed as a</span>
<span class="sd">  weighted sum, or exist in only one metrics, in which case it will be copied.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss_metric_weight_pairs: a list of (metrics, weight) pairs, where each</span>
<span class="sd">      weight is a float and each metrics is a dict with str keys and</span>
<span class="sd">      (metric_value, target_weight) values.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dict with the same set of keys as input metrics and values of</span>
<span class="sd">    (weighted_sum(metric_value), weighted_sum(target_weight)).</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if there exists a metric that exists in more than one element</span>
<span class="sd">      of `loss_metric_weight_pairs` but not in all of them.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">all_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span>
      <span class="n">k</span> <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span>
      <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iterkeys</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>
  <span class="p">])</span>
  <span class="n">result</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">all_keys</span><span class="p">:</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">count</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_metric_weight_pairs</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Found metric </span><span class="si">%s</span><span class="s1"> which exists in more than one&#39;</span>
                       <span class="s1">&#39;but not all loss metrics.&#39;</span> <span class="o">%</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">total_val</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_target_weight</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">loss_metrics</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">loss_metric_weight_pairs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">loss_metrics</span><span class="p">:</span>
        <span class="n">val</span><span class="p">,</span> <span class="n">target_weight</span> <span class="o">=</span> <span class="n">loss_metrics</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="c1"># Single metric, don&#39;t multiply by weight.</span>
          <span class="n">total_val</span> <span class="o">=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">=</span> <span class="n">target_weight</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Total weighted sum of all predictions.</span>
          <span class="n">total_val</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">val</span> <span class="o">*</span> <span class="n">target_weight</span>
          <span class="n">total_target_weight</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">target_weight</span>

    <span class="n">result</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_val</span> <span class="o">/</span> <span class="n">total_target_weight</span><span class="p">,</span> <span class="n">total_target_weight</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="_AddVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._AddVN">[docs]</a><span class="k">def</span> <span class="nf">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">seed</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">seed</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">and</span> <span class="n">step</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">+=</span> <span class="n">step</span> <span class="o">*</span> <span class="mi">203984</span>
  <span class="n">noises</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noises</span></div>


<div class="viewcode-block" id="AddGlobalVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddGlobalVN">[docs]</a><span class="k">def</span> <span class="nf">AddGlobalVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds variational noise to weights if specified by params.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">global_vn</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">weights</span></div>


<div class="viewcode-block" id="AddPerStepVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddPerStepVN">[docs]</a><span class="k">def</span> <span class="nf">AddPerStepVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds per-setp variational noise to weights if specified by params.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">vn</span><span class="o">.</span><span class="n">per_step_vn</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">_AddVN</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">weights</span></div>


<div class="viewcode-block" id="VariationalNoiseParams"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.VariationalNoiseParams">[docs]</a><span class="k">def</span> <span class="nf">VariationalNoiseParams</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span>
                           <span class="n">global_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">per_step_vn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a hyperparams for variational noise.&quot;&quot;&quot;</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span>
      <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span>
      <span class="s1">&#39;Std of the variational noise to apply . This can be a scalar,&#39;</span>
      <span class="s1">&#39; or a scalar tensor.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;global_vn&#39;</span><span class="p">,</span> <span class="n">global_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds global variational noise every training setp iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;per_step_vn&#39;</span><span class="p">,</span> <span class="n">per_step_vn</span><span class="p">,</span>
           <span class="s1">&#39;Adds per-timesetp variational noise iff True.&#39;</span><span class="p">)</span>
  <span class="n">p</span><span class="o">.</span><span class="n">Define</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;Random seed used to generate noise.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p</span></div>


<span class="c1"># To disable VN of a layer, we use 1.0 in the first input parameter</span>
<span class="c1"># of the following function because otherwise it is the same to DefaultVN()</span>
<span class="c1"># configuration of base_layer, which will be updated by parent configuration in</span>
<span class="c1"># CopyBaseParams()</span>
<div class="viewcode-block" id="DisableVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DisableVN">[docs]</a><span class="k">def</span> <span class="nf">DisableVN</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">VariationalNoiseParams</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetStepSeed">[docs]</a><span class="k">def</span> <span class="nf">GetStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Gets step_seed.&quot;&quot;&quot;</span>
  <span class="n">step_seed_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">step_seed_tensors</span><span class="p">:</span>
    <span class="n">ResetStepSeed</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">step_seed_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">step_seed_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Multiple tensors in step_seed collection.&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="ResetStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ResetStepSeed">[docs]</a><span class="k">def</span> <span class="nf">ResetStepSeed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Resets step_seed to specified value.&quot;&quot;&quot;</span>
  <span class="n">new_step_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
  <span class="n">step_seed_tensors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">step_seed_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">step_seed_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_step_seed</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">step_seed_tensors</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s1">&#39;step_seed&#39;</span><span class="p">,</span> <span class="n">new_step_seed</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Multiple tensors in step_seed collection.&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetIncStepSeed"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GetIncStepSeed">[docs]</a><span class="k">def</span> <span class="nf">GetIncStepSeed</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Returns and increments the step_seed.&quot;&quot;&quot;</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="c1"># TODO(lepikhin): introduce a routine filling a queue of uint32 random seeds</span>
  <span class="c1"># independent of underlying PRNG used by tensorflow.</span>
  <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">step_seed</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">step_seed</span></div>


<div class="viewcode-block" id="GenerateStepSeedPair"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.GenerateStepSeedPair">[docs]</a><span class="k">def</span> <span class="nf">GenerateStepSeedPair</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> <span class="n">op_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Generates a seed pair for deterministic random operations in functional loops.</span>

<span class="sd">  This function retrieves a unique seed pair on each call, based off the current</span>
<span class="sd">  global step and step seed. The step seed ensures this function returns a</span>
<span class="sd">  unique seed pair on each call: calling this function automatically increments</span>
<span class="sd">  the step seed. The step seed is automatically reset at the beginning of each</span>
<span class="sd">  global step in the model&#39;s FProp and works transparently through recurrent.py.</span>

<span class="sd">  Args:</span>
<span class="sd">    p: A hyperparams.Params object, containing keys &#39;random_seed&#39; and</span>
<span class="sd">      &#39;is_inference&#39;.</span>
<span class="sd">    global_step: The global step.</span>
<span class="sd">    op_seed: An additional operation-level seed to apply.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A size 2 tensor of op seeds to use for stateless_random ops.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">seed_dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">use_tpu</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">is_inference</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Ensure GetIncStepSeed is called even inside the shortcut.</span>
    <span class="c1"># This ensures if p.random_seed is set for other ops that use this function</span>
    <span class="c1"># that they will get the same seed pair whether or not p.random_seed is set</span>
    <span class="c1"># for this specific call.</span>
    <span class="n">GetIncStepSeed</span><span class="p">()</span>
    <span class="c1"># Unlike tf.random*, stateless random ops are completely determined by the</span>
    <span class="c1"># passed-in seeds. This means at inference time the same inputs will produce</span>
    <span class="c1"># the same outputs, even if the model is supposed to have randomness such as</span>
    <span class="c1"># dropout during inference. We inject additional randomness only during</span>
    <span class="c1"># inference if the graph is exported with random_seed=None as a workaround.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="mi">2</span><span class="p">],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">seed_dtype</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">seed_dtype</span><span class="p">)</span>

  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">step_seed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetIncStepSeed</span><span class="p">(),</span> <span class="n">seed_dtype</span><span class="p">)</span>
  <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">global_step</span><span class="p">,</span> <span class="n">step_seed</span><span class="p">])</span>

  <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">random_seed</span>
  <span class="k">if</span> <span class="n">op_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seeds</span> <span class="o">+=</span> <span class="n">op_seed</span>
  <span class="k">return</span> <span class="n">seeds</span></div>


<div class="viewcode-block" id="DeterministicDropout"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DeterministicDropout">[docs]</a><span class="k">def</span> <span class="nf">DeterministicDropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">noise_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Similar to `tf.nn.dropout()`, but fully deterministic.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A float Tensor on which to apply dropout.</span>
<span class="sd">    keep_prob: A scalar `Tensor` of keep probability.</span>
<span class="sd">    seeds: A Tensor of shape [2]. 2 seeds for deterministic random number</span>
<span class="sd">      generator.</span>
<span class="sd">    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for</span>
<span class="sd">      randomly generated keep/drop flags.</span>
<span class="sd">    name: An optional name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the same shape as `x`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if keep_prob is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
          <span class="s1">&#39;keep_prob must be in range (0, 1]. Value: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">)</span>
    <span class="c1"># uniform in [keep_prob, 1.0 + keep_prob)</span>
    <span class="c1"># StatelessRandomUniform op does not support non-float (e.g. bfloat16) dtype</span>
    <span class="c1"># and non-int32 seed types.</span>
    <span class="n">noise_shape</span> <span class="o">=</span> <span class="n">noise_shape</span> <span class="ow">or</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_uniform</span><span class="p">(</span>
        <span class="n">noise_shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span>
    <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">binary_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">binary_tensor</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">binary_tensor</span>
    <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="DeterministicVN"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.DeterministicVN">[docs]</a><span class="k">def</span> <span class="nf">DeterministicVN</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">noise_shape</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Produces Fully deterministic Gaussian noise from shape, mean and std.</span>

<span class="sd">  Args:</span>
<span class="sd">    params: Nested map of params.</span>
<span class="sd">    seeds: A Tensor of shape [2]. 2 seeds for deterministic random number</span>
<span class="sd">      generator.</span>
<span class="sd">    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for</span>
<span class="sd">      randomly generated Gaussian noise.</span>
<span class="sd">    mean: Mean for the Gaussian noise.</span>
<span class="sd">    std: Standard deviation for noise.</span>
<span class="sd">    name: An optional name for this operation.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor with the shape noise_shape and type fprop_dtype.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;gaussian_noise&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
      <span class="n">seeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="p">(</span>
        <span class="n">std</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">stateless_normal</span><span class="p">(</span><span class="n">noise_shape</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">,</span> <span class="n">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">random_tensor</span></div>


<span class="n">BATCH_NORM_UPDATES</span> <span class="o">=</span> <span class="s1">&#39;batch_norm_updates&#39;</span>

<span class="n">_BATCH_NORM_UPDATES_DICT</span> <span class="o">=</span> <span class="s1">&#39;__batch_norm_update_dict&#39;</span>
<span class="n">_get_batch_norm_updates_dict</span> <span class="o">=</span> <span class="n">_CollectionGetter</span><span class="p">(</span><span class="n">_BATCH_NORM_UPDATES_DICT</span><span class="p">,</span>
                                                 <span class="k">lambda</span><span class="p">:</span> <span class="p">{})</span>


<div class="viewcode-block" id="UpdateBatchNormVars"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateBatchNormVars">[docs]</a><span class="k">def</span> <span class="nf">UpdateBatchNormVars</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Update batch normalization moving averages.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
      <span class="s1">&#39;AssignMovingAvg&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span>
          <span class="n">batch_norm_var</span><span class="p">,</span>
          <span class="n">batch_norm_stats</span><span class="p">,</span>
          <span class="n">decay</span><span class="p">,</span>
      <span class="p">])</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">):</span>
      <span class="n">decay</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
          <span class="mf">1.0</span> <span class="o">-</span> <span class="n">decay</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">batch_norm_var</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">)</span>
      <span class="n">update_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span> <span class="o">-</span> <span class="n">batch_norm_stats</span><span class="p">)</span> <span class="o">*</span> <span class="n">decay</span>
      <span class="n">bn_update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">update_delta</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">BATCH_NORM_UPDATES</span><span class="p">,</span> <span class="n">bn_update</span><span class="p">)</span>
  <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
  <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bn_update_dict</span>
  <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_norm_var</span><span class="p">,</span> <span class="n">batch_norm_stats</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">bn_update</span></div>


<div class="viewcode-block" id="FindRelevantBatchNormUpdates"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FindRelevantBatchNormUpdates">[docs]</a><span class="k">def</span> <span class="nf">FindRelevantBatchNormUpdates</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">batch_norm_updates</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Finds and returns a list of relevant batch-normalization updates.</span>

<span class="sd">  Args:</span>
<span class="sd">    loss: The loss that is being optimized for. A tensor or a list of tensors.</span>
<span class="sd">    batch_norm_updates: A list of batch normalization updates.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pair of lists. The first list contains all the batch normalization updates</span>
<span class="sd">    that are relevant to the loss being optimized, and the second list contains</span>
<span class="sd">    all in batch_norm_updates but not in the first list.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dependent_ops_and_tensors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">FindNeeded</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
  <span class="n">relevant_updates</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">irrelevant_updates</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">bn_update_dict</span> <span class="o">=</span> <span class="n">_get_batch_norm_updates_dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">bn_update</span> <span class="ow">in</span> <span class="n">batch_norm_updates</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">bn_update</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">bn_update_dict</span><span class="p">,</span> <span class="p">(</span>
        <span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> is probably not a valid batch normalization update op.&#39;</span>
        <span class="s1">&#39; Make sure batch normalization is done through calling&#39;</span>
        <span class="s1">&#39; the py_utils.UpdateBatchNormVars helper routine.&#39;</span><span class="p">)</span>
    <span class="n">bn_stat_name</span> <span class="o">=</span> <span class="n">bn_update_dict</span><span class="p">[</span><span class="n">bn_update</span><span class="o">.</span><span class="n">name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="n">bn_stat_name</span> <span class="ow">in</span> <span class="n">dependent_ops_and_tensors</span><span class="p">:</span>
      <span class="c1"># If a batch normalization stat is computed in the forward pass in</span>
      <span class="c1"># computing loss, then the corresponding batch normalization update is</span>
      <span class="c1"># relevant. Otherwise, it is not.</span>
      <span class="n">relevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">irrelevant_updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bn_update</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">relevant_updates</span><span class="p">,</span> <span class="n">irrelevant_updates</span></div>


<span class="n">_SAMPLE_STEP_KEY</span> <span class="o">=</span> <span class="s1">&#39;sample_step&#39;</span>


<div class="viewcode-block" id="SampleStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SampleStep">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">SampleStep</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A context for a sample step during decoding.</span>

<span class="sd">  Example usage::</span>

<span class="sd">      with py_utils.SampleStep(step):</span>
<span class="sd">        sample = self.DecodeOneStep()</span>

<span class="sd">  Args:</span>
<span class="sd">    step: the step tensor.</span>

<span class="sd">  Yields:</span>
<span class="sd">    a context manager for the step scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">stack</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection_ref</span><span class="p">(</span><span class="n">_SAMPLE_STEP_KEY</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">step</span>
  <span class="k">finally</span><span class="p">:</span>
    <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span></div>


<div class="viewcode-block" id="_GetSampleStep"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._GetSampleStep">[docs]</a><span class="k">def</span> <span class="nf">_GetSampleStep</span><span class="p">():</span>
  <span class="n">stack</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">_SAMPLE_STEP_KEY</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">stack</span> <span class="k">else</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="AddDebugTensor"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.AddDebugTensor">[docs]</a><span class="k">def</span> <span class="nf">AddDebugTensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds `tensor` to the debug collection.</span>

<span class="sd">  Prints the tensor if `--print_debug_tensors` is True.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A tensor.</span>
<span class="sd">    summarize: Only print this many entries of each tensor. If None, then a</span>
<span class="sd">      maximum of 3 elements are printed per input tensor.</span>
<span class="sd">    name: An optional name for the tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor that evaluates to the same value as the input tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">_FromGlobal</span><span class="p">(</span><span class="s1">&#39;print_debug_tensors&#39;</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">_GetSampleStep</span><span class="p">()</span>
    <span class="n">tensors_to_print</span> <span class="o">=</span> <span class="p">([]</span> <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="n">step</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">s</span><span class="p">:</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Print</span><span class="p">(</span>
          <span class="n">tensor</span><span class="p">,</span>
          <span class="n">tensors_to_print</span><span class="p">,</span>
          <span class="n">message</span><span class="o">=</span><span class="s1">&#39;DEBUG tensor </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">s</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="ArgMax"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ArgMax">[docs]</a><span class="k">def</span> <span class="nf">ArgMax</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.argmax wrapper.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: A tensor, whose last dimension is being reduced on.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor of rank tf.rank(logits)-1. If i == ret[indices],</span>
<span class="sd">    logits[indices, i] is the maximum among logits[indices, :].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="_EnsureMatrixShape"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._EnsureMatrixShape">[docs]</a><span class="k">def</span> <span class="nf">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">==</span> <span class="mi">2</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Matmul"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Matmul">[docs]</a><span class="k">def</span> <span class="nf">Matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;tf.matmul wrapper expecting x and y are actually matrices.&quot;&quot;&quot;</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">_EnsureMatrixShape</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="clip_by_value"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.clip_by_value">[docs]</a><span class="k">def</span> <span class="nf">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=invalid-name</span>
  <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_real&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">_imag&#39;</span> <span class="o">%</span> <span class="n">name</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="_TransformAndSum"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._TransformAndSum">[docs]</a><span class="k">def</span> <span class="nf">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">transform</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;TransformAndSum&#39;</span><span class="p">):</span>
    <span class="n">sum_transform</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">values</span><span class="p">))]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">sum_transform</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">transform</span><span class="p">(</span><span class="n">t</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">sum_transform</span><span class="p">)</span></div>


<div class="viewcode-block" id="SumSquared"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SumSquared">[docs]</a><span class="k">def</span> <span class="nf">SumSquared</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="SumAbs"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SumAbs">[docs]</a><span class="k">def</span> <span class="nf">SumAbs</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_TransformAndSum</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">)</span></div>


<div class="viewcode-block" id="PiecewiseConstant"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PiecewiseConstant">[docs]</a><span class="k">def</span> <span class="nf">PiecewiseConstant</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">vdtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the piecewise value of x_in.&quot;&quot;&quot;</span>
  <span class="n">x_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x_in</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span>
  <span class="n">bs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">vs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="c1"># The following is equivalent to &#39;return vs[index]&#39;.</span>
  <span class="n">index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
  <span class="n">one_hot_vec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">depth</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vdtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">one_hot_vec</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="PadSequenceDimension"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadSequenceDimension">[docs]</a><span class="k">def</span> <span class="nf">PadSequenceDimension</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads x to `length` using `pad_val` along the second dim.</span>

<span class="sd">  Assumes `x` is a tensor with rank &gt;= 2, and it only pads `x` to `length`</span>
<span class="sd">  along the second dim. Explicitly sets the returned tensor shape to `shape` if</span>
<span class="sd">  given. Raises runtime errors if x.shape[1] &gt; length or x.shape[i] != shape[i]</span>
<span class="sd">  where i != 1.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: the tensor to be padded with shape [batch, seq_len, ...].</span>
<span class="sd">    length: an int to specify the length to pad x to.</span>
<span class="sd">    pad_val: an int or float used to pad x.</span>
<span class="sd">    shape: an int array specifying the shape of the padded tensor if specified.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The padded tensor with shape [batch, seq_len, ...], where</span>
<span class="sd">    ret[:, :seq_len, ...] == x.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span>
    <span class="k">assert</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="n">slen</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rank</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rank</span><span class="p">)]</span>
    <span class="n">pad</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pad_len</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]):</span>
      <span class="n">slen</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pad_len</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="n">slen</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">pad_len</span><span class="p">],</span> <span class="p">[</span><span class="n">rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">static_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">static_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span>
    <span class="n">x</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">static_shape</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">shape</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Shape must be a list or tuple.&#39;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ensure_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="PadSequenceTo"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadSequenceTo">[docs]</a><span class="k">def</span> <span class="nf">PadSequenceTo</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pads `xs` and `padding` to `length` using `pad_val` along the 2nd dim.</span>

<span class="sd">  Pads `xs` to `length` using `pad_val`, and `padding` using 1.</span>
<span class="sd">  Raise error if `x.shape[:2]` and `padding.shape` are not the same.</span>

<span class="sd">  Args:</span>
<span class="sd">    xs: A Tensor or a list of Tensors of shape [batch, seqlen] or [batch,</span>
<span class="sd">      seqlen, ...].</span>
<span class="sd">    padding: A 0/1 Tensor of shape [batch, seqlen]. 1 is for padded locations.</span>
<span class="sd">    length: A Python int, the length to pad to.</span>
<span class="sd">    pad_val: A Python numeric, used for padding x.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of padded xs and padding.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">new_xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">xs</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">new_xs</span> <span class="o">=</span> <span class="n">xs</span>

  <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">new_xs</span><span class="p">:</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">slen</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">padding</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">HasShape</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">slen</span><span class="p">])</span>

    <span class="n">new_x</span> <span class="o">=</span> <span class="n">PadSequenceDimension</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">pad_val</span><span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">PadSequenceDimension</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">padding</span></div>


<div class="viewcode-block" id="ApplyPadding"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ApplyPadding">[docs]</a><span class="k">def</span> <span class="nf">ApplyPadding</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">padded</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_select</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies padding to a tensor.</span>

<span class="sd">  This is preferable to using arithmetic means for masking out padded values</span>
<span class="sd">  such as::</span>

<span class="sd">      # Equiv to ApplyPadding(padding, x))</span>
<span class="sd">      x *= 1.0 - padding</span>
<span class="sd">      # Equiv to ApplyPadding(padding, new, old)</span>
<span class="sd">      new = old * padding + new * (1 - padding)</span>

<span class="sd">  Aside from just being easier to read and reason about, using this function</span>
<span class="sd">  is friendly to quantized representations because it does not mix arithmetic</span>
<span class="sd">  on the padding values with the values in the tensor being padded (which can</span>
<span class="sd">  have a very different range than the 0..1 padding tensor).</span>

<span class="sd">  In addition, this works around issues in quantized schemes where we are</span>
<span class="sd">  guaranteed to have an exact 0 but not necessarily any other number (i.e. 1).</span>

<span class="sd">  Args:</span>
<span class="sd">    padding: Tensor of padding values where 0 == keep and 1 == pad.</span>
<span class="sd">    x: Tensor to apply padding to.</span>
<span class="sd">    padded: Optional. Values to include for padded elements. Defaults to zeros.</span>
<span class="sd">      Must be the same shape as &#39;x&#39; if specified.</span>
<span class="sd">    broadcast: Whether to broadcast the padding shape to the shape of &#39;x&#39;. You</span>
<span class="sd">      almost certainly want this to be true as it matches how padding would be</span>
<span class="sd">      expanded if applied arithmetically.</span>
<span class="sd">    use_select: Controls whether padding is applied with a select-mask</span>
<span class="sd">      (True/default) or arithmetically (False). Some platforms have a</span>
<span class="sd">      sensitivity to one or the other and this is used to work around such</span>
<span class="sd">      issues.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tensor with the same shape as x with padded values masked.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">Assert</span><span class="p">(</span>
          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))),</span>
          <span class="p">[</span><span class="n">padding</span><span class="p">])</span>
  <span class="p">],</span> <span class="n">padding</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">use_select</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">padded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">broadcast</span><span class="p">:</span>
      <span class="c1"># Broadcast padding to the full shape.</span>
      <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padding</span> <span class="o">&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">padded</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padded</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">+=</span> <span class="n">padded</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padded</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="LengthsFromPaddings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.LengthsFromPaddings">[docs]</a><span class="k">def</span> <span class="nf">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes lengths of each sequence in a batch, ignoring trailing padding.</span>

<span class="sd">  Args:</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    lengths tensor shaped [batch] containing the unpadded length of each</span>
<span class="sd">    sequence in the batch.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="c1"># Find the last unpadded value.</span>
  <span class="c1"># Cannot just use tf.reduce_sum because there might be leading paddings.</span>
  <span class="c1"># Everything after the last unpadded value has 1.0 - paddings == 0.0, so in</span>
  <span class="c1"># the cumsum below they will have the same value.</span>
  <span class="n">cumsum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">same_as_last_element</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">cumsum</span><span class="p">,</span> <span class="n">cumsum</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:])</span>
  <span class="c1"># Counting the number of elements with the same value gives us num_padded + 1</span>
  <span class="c1"># and so counting the number that differs gives us num_padded - 1.</span>
  <span class="n">length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
      <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">same_as_last_element</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="c1"># Special case for all 0 paddings.</span>
  <span class="n">all_zero_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">all_zero_paddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">length</span><span class="p">),</span> <span class="n">length</span><span class="p">)</span></div>


<div class="viewcode-block" id="TrimTrailingPaddings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TrimTrailingPaddings">[docs]</a><span class="k">def</span> <span class="nf">TrimTrailingPaddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Trims trailing paddings from inputs.</span>

<span class="sd">  Since the number of dimensions is not fixed, this will not work on TPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor with shape [batch, length, ...].</span>
<span class="sd">    paddings: a tensor with shape [batch, length].</span>

<span class="sd">  Returns:</span>
<span class="sd">    Trimmed inputs and paddings. For compatibility reasons, the trimmed tensors</span>
<span class="sd">    will always have length at least 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">paddings</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">max_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">paddings</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">],</span> <span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">output_shape</span><span class="p">),</span> <span class="n">output_shape</span><span class="p">)</span>
  <span class="n">out_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">max_length</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">out_paddings</span></div>


<div class="viewcode-block" id="ReversePaddedSequence"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReversePaddedSequence">[docs]</a><span class="k">def</span> <span class="nf">ReversePaddedSequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Reverse inputs based on paddings.</span>

<span class="sd">  Only reverse the unpadded portion of `inputs`. It assumes inputs are only</span>
<span class="sd">  padded in the end.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a tensor of [seq_length, batch_size, num_input_nodes].</span>
<span class="sd">    paddings: a tensor of float32/float64 zero or one of shape [seq_length,</span>
<span class="sd">      batch_size, 1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    A reversed tensor of the same shape as `inputs`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inversed_paddings</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">inputs_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">inversed_paddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_length</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConcatenatePaddedSequences"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ConcatenatePaddedSequences">[docs]</a><span class="k">def</span> <span class="nf">ConcatenatePaddedSequences</span><span class="p">(</span><span class="n">input0</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">padding0</span><span class="p">,</span> <span class="n">padding1</span><span class="p">,</span> <span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concatenates input sequences with varying lenghts as defined by paddings.</span>

<span class="sd">  This is a helper function for concatenating 2 batches of input sequences,</span>
<span class="sd">  where each example in the batch can have different lengths, as defined by</span>
<span class="sd">  the corresponding paddings. To concatenate correctly, it makes use of</span>
<span class="sd">  tf.reverse_sequence to partially reverse the sequences before</span>
<span class="sd">  concatenating them together.</span>

<span class="sd">  NOTE: We assume that the tensors have no leading paddings.</span>

<span class="sd">  Args:</span>
<span class="sd">    input0: A tensor of size [batch, max_length, ...] or [max_length, batch,</span>
<span class="sd">      ...] depending on the value set for axis.</span>
<span class="sd">    input1:  A tensor of size [batch, max_length, ...] or [max_length, batch,</span>
<span class="sd">      ...] depending on the value set for axis.</span>
<span class="sd">    padding0: A Tensor of size [batch, max_length] or [max_length, batch]</span>
<span class="sd">      corresponding to the padding for input0.</span>
<span class="sd">    padding1: A Tensor of size [batch, max_length] or [max_length, batch]</span>
<span class="sd">      corresponding to the padding for input1.</span>
<span class="sd">    seq_dim: int, the time axis along which the tensors will be concatenated.</span>
<span class="sd">      Should be 0 or 1. Assumes that batch_dim is 1 - seq_dim.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The concatenation of input0 and input1, and the corresponding padding.</span>

<span class="sd">  Raises:</span>
<span class="sd">    tf.errors.InvalidArgumentError when seq_dim is not 0 or 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">seq_dim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;seq_dim must be 0 or 1.&#39;</span><span class="p">)</span>
  <span class="n">batch_dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">seq_dim</span>
  <span class="c1"># inpu0 and input1 should have the same batch size and same rank.</span>
  <span class="n">input0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">input0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span>
                   <span class="n">GetShape</span><span class="p">(</span><span class="n">input1</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">]),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetRank</span><span class="p">(</span><span class="n">input0</span><span class="p">),</span> <span class="n">GetRank</span><span class="p">(</span><span class="n">input1</span><span class="p">))</span>
  <span class="p">],</span> <span class="n">input0</span><span class="p">)</span>

  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">padding0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">]</span>
  <span class="c1"># batch dimension of inputs and paddings should match.</span>
  <span class="n">input0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">input0</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">padding1</span><span class="p">)[</span><span class="n">batch_dim</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">input0</span><span class="p">)</span>
  <span class="n">input0_seq_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">to_int32</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">padding0</span><span class="p">)[</span><span class="n">seq_dim</span><span class="p">]],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">]))</span>
  <span class="n">input1_seq_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">to_int32</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">padding1</span><span class="p">)[</span><span class="n">seq_dim</span><span class="p">]],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">]))</span>
  <span class="c1"># LengthsFromPaddings assumes that paddings is of size [batch, max_length].</span>
  <span class="k">if</span> <span class="n">seq_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">padding0</span><span class="p">)</span>
    <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">padding1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">padding0</span><span class="p">))</span>
    <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">LengthsFromPaddings</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">padding1</span><span class="p">))</span>
  <span class="c1"># We assume that the tensors have no leading paddings.</span>
  <span class="c1"># TODO(arunnt): Concatenate tensors with leading paddings correctly.</span>
  <span class="n">seq_length0</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">seq_length0</span><span class="p">,</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">to_int32</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding0</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">)))</span>
  <span class="p">],</span> <span class="n">seq_length0</span><span class="p">)</span>
  <span class="n">seq_length1</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">seq_length1</span><span class="p">,</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">to_int32</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">padding1</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">)))</span>
  <span class="p">],</span> <span class="n">seq_length1</span><span class="p">)</span>
  <span class="c1"># Concatenate input sequences.</span>
  <span class="n">reversed_input0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">input0</span><span class="p">,</span> <span class="n">seq_length0</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_input1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">input1</span><span class="p">,</span> <span class="n">input1_seq_dim</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_concat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">reversed_input1</span><span class="p">,</span> <span class="n">reversed_input0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">)</span>
  <span class="n">concat_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">reversed_concat</span><span class="p">,</span>
      <span class="n">seq_length0</span> <span class="o">+</span> <span class="n">input1_seq_dim</span><span class="p">,</span>
      <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span>
      <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="c1"># Concatenate paddings. Note that paddings are always a Tensor of 0s and 1s,</span>
  <span class="c1"># so, unlike the inputs, we don&#39;t have to reverse padding1, we can simply</span>
  <span class="c1"># concatenate reversed padding0 and padding1.</span>
  <span class="n">reversed_padding0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">padding0</span><span class="p">,</span> <span class="n">input0_seq_dim</span><span class="p">,</span> <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="n">reversed_concat_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">reversed_padding0</span><span class="p">,</span> <span class="n">padding1</span><span class="p">],</span>
                                      <span class="n">axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">)</span>
  <span class="n">concat_paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span>
      <span class="n">reversed_concat_padding</span><span class="p">,</span>
      <span class="n">input0_seq_dim</span> <span class="o">+</span> <span class="n">seq_length1</span><span class="p">,</span>
      <span class="n">seq_axis</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span>
      <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">concat_inputs</span><span class="p">,</span> <span class="n">concat_paddings</span></div>


<div class="viewcode-block" id="Retry"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.Retry">[docs]</a><span class="k">def</span> <span class="nf">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">retry</span><span class="o">.</span><span class="n">Retry</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<span class="c1"># FailedPreconditionError: variables are not initialized.</span>
<span class="c1"># AbortedError: processes restarts.</span>
<span class="c1"># UnavailableError: Bad hardware status: 0x1</span>
<span class="n">transient_tf_errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">FailedPreconditionError</span><span class="p">,</span>
                       <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">AbortedError</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">UnavailableError</span><span class="p">)</span>


<div class="viewcode-block" id="RetryOnTransientTfError"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RetryOnTransientTfError">[docs]</a><span class="k">def</span> <span class="nf">RetryOnTransientTfError</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">Retry</span><span class="p">(</span><span class="n">transient_tf_errors</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="PadOrTrimTo"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.PadOrTrimTo">[docs]</a><span class="k">def</span> <span class="nf">PadOrTrimTo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">pad_val</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Pad and slice x to the given shape.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A tensor.</span>
<span class="sd">    shape: The shape of the returned tensor.</span>
<span class="sd">    pad_val: An int or float used to pad x.</span>

<span class="sd">  Returns:</span>
<span class="sd">    &#39;x&#39; is padded with pad_val and sliced so that the result has the given</span>
<span class="sd">    shape.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if shape is a tf.TensorShape and not fully defined.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shape</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1"> padding </span><span class="si">%s</span><span class="s1"> must be fully defined.&#39;</span> <span class="o">%</span>
                       <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">expected_rank</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">HasRank</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">expected_rank</span><span class="p">)</span>
  <span class="c1"># If dim-i is less than shape[i], pads on the right shape[i] -</span>
  <span class="c1"># dim-i.  Otherwise, pads [0, 0] for dim-i.</span>
  <span class="n">pad</span> <span class="o">=</span> <span class="n">shape</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
  <span class="n">zeros</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">zeros</span><span class="p">,</span> <span class="n">pad</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">constant_values</span><span class="o">=</span><span class="n">pad_val</span><span class="p">)</span>
  <span class="c1"># If dim-i is larger than shape[i], we slice [0:shape[i]] for dim-i.</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="RepeatDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RepeatDim">[docs]</a><span class="k">def</span> <span class="nf">RepeatDim</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">multiple</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copies elements in tensor&#39;s axis &quot;multiple&quot; times, like np.repeat.&quot;&quot;&quot;</span>
  <span class="c1"># x = [[1, 2, 3], [4, 5, 6]]</span>
  <span class="c1"># RepeatDim(x, multiple=2, axis=1) gives:</span>
  <span class="c1"># [[1, 1, 2, 2, 3, 3]. [4, 4, 5, 5, 6, 6]]</span>
  <span class="c1"># As a comparison tf.tile(x, multiples=[1, 2]) gives:\</span>
  <span class="c1"># [[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]</span>

  <span class="k">if</span> <span class="n">multiple</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tensor</span>
  <span class="n">t_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">tensor_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">t_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiple</span><span class="p">],</span> <span class="n">t_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">multiple_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">multiple</span><span class="p">],</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
  <span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">multiple_dims</span><span class="p">),</span> <span class="n">tensor_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="StackTensorsRecursively"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.StackTensorsRecursively">[docs]</a><span class="k">def</span> <span class="nf">StackTensorsRecursively</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively stacks Tensors in a list of `.NestedMap`.</span>

<span class="sd">  Args:</span>
<span class="sd">    values: a list of `.NestedMap` or Tensors to stacks.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `.NestedMap` with stacked values or a stacked Tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">flatten</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
  <span class="n">stacked</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">stacked</span> <span class="o">+=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">flatten</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">flatten</span><span class="p">))])]</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">stacked</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="MixByWeight"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.MixByWeight">[docs]</a><span class="k">def</span> <span class="nf">MixByWeight</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a weighted random choice and bprop type from the give inputs.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: a list of callables, where each callable returns a tf.Tensor or a</span>
<span class="sd">      nested structure containing tf.Tensor. Function return types must be</span>
<span class="sd">      consistent across elements. The tf.Operation to compute the result tensor</span>
<span class="sd">      will only be invoked for one input at a time. For example, if each fn</span>
<span class="sd">      represents an input record stream, a record will be drawn only from a</span>
<span class="sd">      selected stream while the other streams will remain unchanged.</span>
<span class="sd">    weights: a 1D tensor of float &gt; 0 of the same length as inputs.</span>
<span class="sd">    seed: random seed.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A probablistic sample from the inputs proportional to the weights. The</span>
<span class="sd">    return type will be the same as return type of individual &#39;fn&#39; from the</span>
<span class="sd">    inputs.</span>
<span class="sd">    A one-hot vector of the source selected.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]),</span>
      <span class="n">assert_greater_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">weights</span><span class="p">)</span>

  <span class="n">lower</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">upper</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">upper</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">return_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">selected_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span>
      <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">r</span><span class="p">,</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="k">lambda</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="p">)</span>
       <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))],</span>
      <span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">bprop_index</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">selected_index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">return_input</span><span class="p">,</span> <span class="n">bprop_index</span></div>


<div class="viewcode-block" id="CheckShapes"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CheckShapes">[docs]</a><span class="k">def</span> <span class="nf">CheckShapes</span><span class="p">(</span><span class="n">shapes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Asserts that shapes is a tuple of NestedMap or tshape.Shape.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">NestedMap</span><span class="p">):</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                 <span class="p">]),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> contains non-tensor value.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tshape</span><span class="o">.</span><span class="n">Shape</span><span class="p">),</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">s</span><span class="p">)</span></div>


<div class="viewcode-block" id="FPropDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.FPropDtype">[docs]</a><span class="k">def</span> <span class="nf">FPropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">params</span><span class="o">.</span><span class="n">dtype</span></div>


<div class="viewcode-block" id="UpdateFpropDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateFpropDtype">[docs]</a><span class="k">def</span> <span class="nf">UpdateFpropDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the fprop_dtype of the Params.&quot;&quot;&quot;</span>
  <span class="c1"># Handle the case when the input &quot;params&quot; is not an instance of hyperparams</span>
  <span class="c1"># For example, when UpdateDtype is called recursively for all the items in</span>
  <span class="c1"># the &quot;sub&quot; list of SequentialLayer (see 1st elif below)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
    <span class="k">return</span>

  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateFpropDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">val</span><span class="p">:</span>
        <span class="n">UpdateFpropDtype</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">fprop_dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;fprop_dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">fprop_dtype</span> <span class="o">=</span> <span class="n">fprop_dtype</span></div>


<div class="viewcode-block" id="UpdateDtype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.UpdateDtype">[docs]</a><span class="k">def</span> <span class="nf">UpdateDtype</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursively update the dtype of the Params.&quot;&quot;&quot;</span>
  <span class="c1"># Handle the case when the input &quot;params&quot; is not an instance of hyperparams</span>
  <span class="c1"># For example, when UpdateDtype is called recursively for all the items in</span>
  <span class="c1"># the &quot;sub&quot; list of SequentialLayer (see 1st elif below)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
    <span class="k">return</span>

  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">IterParams</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">hyperparams</span><span class="o">.</span><span class="n">Params</span><span class="p">):</span>
      <span class="n">UpdateDtype</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">val</span><span class="p">:</span>
        <span class="n">UpdateDtype</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">key</span> <span class="o">==</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span>
      <span class="n">params</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span></div>


<div class="viewcode-block" id="NameScopeDecorator"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.NameScopeDecorator">[docs]</a><span class="k">def</span> <span class="nf">NameScopeDecorator</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Decorates a python function to introduce a tf.name_scope.</span>

<span class="sd">  Example::</span>

<span class="sd">      @py_utils.NameScopeDecorator(&#39;foobar&#39;)</span>
<span class="sd">      def MyFoobarMethod(self):</span>
<span class="sd">        # ... Do TF things</span>

<span class="sd">  Args:</span>
<span class="sd">    name_scope: The name scope to introduce.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function decorator.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">Decorator</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">Wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name_scope</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Wrapped</span>

  <span class="k">return</span> <span class="n">Decorator</span></div>


<div class="viewcode-block" id="SequencesToDebugStrings"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SequencesToDebugStrings">[docs]</a><span class="k">def</span> <span class="nf">SequencesToDebugStrings</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns debug strings for the given sequences.</span>

<span class="sd">  Args:</span>
<span class="sd">    ids: int32 of [batch, len].</span>
<span class="sd">    lens: int32 of [batch].</span>
<span class="sd">    summarize: number of ids to summarize per sequence.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A string tensor of [batch].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_seqs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">lens</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_Body</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">strings</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">lens</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">summarize</span><span class="o">=</span><span class="n">summarize</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">result</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">i0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">result0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">strs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_seqs</span><span class="p">,</span>
      <span class="n">_Body</span><span class="p">,</span> <span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">result0</span><span class="p">),</span>
      <span class="n">shape_invariants</span><span class="o">=</span><span class="p">(</span><span class="n">i0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">])))</span>
  <span class="k">return</span> <span class="n">strs</span></div>


<div class="viewcode-block" id="RematerializeFn"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RematerializeFn">[docs]</a><span class="k">def</span> <span class="nf">RematerializeFn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">xs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calls fn and rematerializes fn in the backward pass.</span>

<span class="sd">  `fn(*xs) -&gt; ys`, where xs and ys can be a single tensor or a tuple of tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    fn: A python function to be rematerialized in the backprop pass.</span>
<span class="sd">    *xs: A single tensor or a list/tuple of tensors. `xs` are input args to the</span>
<span class="sd">      fn function.</span>

<span class="sd">  Returns:</span>
<span class="sd">    `fn(*xs)`</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">initial_step_seed</span> <span class="o">=</span> <span class="n">GetStepSeed</span><span class="p">()</span>
  <span class="n">final_step_seed</span> <span class="o">=</span> <span class="n">GenerateSeedFromName</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;new_step_seed&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">dy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The backward function that rematerializes forward outputs.&quot;&quot;&quot;</span>
    <span class="n">always_true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([])</span> <span class="o">&lt;</span> <span class="mf">2.0</span>
    <span class="c1"># Alternatively, can do this:</span>
    <span class="c1"># tf.where(tf.is_nan(x),</span>
    <span class="c1">#          tf.constant(float(&#39;nan&#39;), dtype=x.dtype) * tf.ones_like(x),</span>
    <span class="c1">#          x)</span>
    <span class="c1"># Skip op.inputs[0] which is initial_step_seed.</span>
    <span class="n">bak_xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">always_true</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
    <span class="n">dxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="n">dy</span><span class="p">)</span>
    <span class="n">dxs_final</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">dx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dxs</span><span class="p">,</span> <span class="n">bak_xs</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">dx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">dxs_final</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dxs_final</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">bak_xs</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">),)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">dxs_final</span><span class="p">)</span>

  <span class="n">xs_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
  <span class="n">ys_shapes</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># TODO(huangyp, yonghui): Check Forward doesn&#39;t use any stateful random ops.</span>
  <span class="nd">@tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="o">*</span><span class="n">xs_dtypes</span><span class="p">,</span> <span class="n">python_grad_func</span><span class="o">=</span><span class="n">Backward</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">,</span> <span class="o">*</span><span class="n">fwd_xs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Forward function plus sanity checks.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fwd_xs</span><span class="p">,</span> <span class="n">xs</span><span class="p">):</span>
      <span class="n">dst</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">fwd_xs</span><span class="p">)</span>
    <span class="c1"># Some sanity check.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_inputs</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_args</span><span class="p">()</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">function</span><span class="o">.</span><span class="n">get_extra_vars</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
      <span class="n">ys_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>

  <span class="n">ys</span> <span class="o">=</span> <span class="n">Forward</span><span class="p">(</span><span class="n">initial_step_seed</span><span class="p">,</span> <span class="o">*</span><span class="n">xs</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">ys_shapes</span><span class="p">):</span>
      <span class="n">y</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">ys_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="c1"># TODO(b/129159299): The ResetStepSeed below is needed to work around this</span>
  <span class="c1"># bug, which is a problem with global tensors being shared by different</span>
  <span class="c1"># inference graphs. It should be replaced with the new step seed value</span>
  <span class="c1"># returned from the Forward function when the bug is fixed.</span>
  <span class="n">ResetStepSeed</span><span class="p">(</span><span class="n">final_step_seed</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ys</span></div>


<span class="c1"># A set of names of stateful random number generator ops.</span>
<span class="c1"># See tensorflow/core/ops/random_ops.cc</span>
<span class="n">_STATEFUL_RANDOM_OPS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># pyformat: disable</span>
    <span class="s1">&#39;RandomUniform&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomUniformInt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomStandardNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ParameterizedTruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TruncatedNormal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomShuffle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Multinomial&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomGamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoisson&#39;</span><span class="p">,</span>
    <span class="s1">&#39;RandomPoissonV2&#39;</span><span class="p">,</span>
    <span class="c1"># pyformat: enable</span>
<span class="p">}</span>


<div class="viewcode-block" id="StatefulRandomOpsInDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.StatefulRandomOpsInDefun">[docs]</a><span class="k">def</span> <span class="nf">StatefulRandomOpsInDefun</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Checks whether the Defun depends on stateful random number ops.</span>

<span class="sd">  Stateful random number generator ops should be avoid in Recurrent() call.</span>
<span class="sd">  Otherwise, these ops produce inconsistent values between FProp and BProp.</span>

<span class="sd">  Args:</span>
<span class="sd">    func: a _DefinedFunction to check.</span>
<span class="sd">    graph: a Graph. Set None to use the default graph.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of names of the stateful random ops.</span>

<span class="sd">  Raises:</span>
<span class="sd">    InvalidArgumentError: if the input func/graph is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">function</span><span class="o">.</span><span class="n">_DefinedFunction</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                         <span class="s1">&#39;func is not a _DefinedFunction.&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
  <span class="n">func</span><span class="o">.</span><span class="n">add_to_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
  <span class="n">graph_def</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>

  <span class="c1"># A dict from function name to FunctionDef.</span>
  <span class="n">func_defs</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">graph_def</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">function</span><span class="p">}</span>

  <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">(</span>
        <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;Defun </span><span class="si">{}</span><span class="s1"> is not in the graph .&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

  <span class="n">stateful_ops</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Recursively search for stateful random op.</span>
  <span class="n">nodes</span> <span class="o">=</span> <span class="n">py_collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">definition</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>
  <span class="k">while</span> <span class="n">nodes</span><span class="p">:</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="p">),</span> <span class="n">node</span>

    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="n">_STATEFUL_RANDOM_OPS</span><span class="p">:</span>
      <span class="n">stateful_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
      <span class="k">continue</span>

    <span class="k">def</span> <span class="nf">_AddDefunNodes</span><span class="p">(</span><span class="n">func_name</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;If the given func_name is a Defun, add its sub-nodes into nodes.&quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">func_name</span> <span class="ow">in</span> <span class="n">func_defs</span><span class="p">:</span>
        <span class="n">nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">func_defs</span><span class="p">[</span><span class="n">func_name</span><span class="p">]</span><span class="o">.</span><span class="n">node_def</span><span class="p">)</span>

    <span class="c1"># For functional.{While|For|If} ops, add their Defun attr into search.</span>
    <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;While&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;cond&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;For&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;If&#39;</span><span class="p">:</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;then_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;else_branch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># For other op, check whether itself is a Defun op.</span>
      <span class="n">_AddDefunNodes</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">stateful_ops</span></div>


<div class="viewcode-block" id="ToPlaceholders"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ToPlaceholders">[docs]</a><span class="k">def</span> <span class="nf">ToPlaceholders</span><span class="p">(</span><span class="n">nmap</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts every Tensor in nmap to a placeholder.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_ToPlacerholder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">nmap</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="n">_ToPlacerholder</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyFocalLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SoftmaxCrossEntropyFocalLoss">[docs]</a><span class="k">def</span> <span class="nf">SoftmaxCrossEntropyFocalLoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span>
                                 <span class="n">label_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">label_probs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">u</span><span class="sd">&quot;&quot;&quot;Focal loss for multinomial (softmax) logistic loss.</span>

<span class="sd">  [1] Focal loss https://arxiv.org/abs/1708.02002</span>

<span class="sd">  Args:</span>
<span class="sd">    logits: [..., C]. Logits for the multinomial logistic regression. C is the</span>
<span class="sd">      number of classes.</span>
<span class="sd">    label_ids: [...]. Each entry in labels must be an index in [0, C).</span>
<span class="sd">    label_probs: [..., C]. Each vector along last dimension must be a valid</span>
<span class="sd">      probability distribution.</span>
<span class="sd">    alpha: [C]. The weighting factor alpha. Eq (3) in [1].</span>
<span class="sd">    gamma: []. Tunable focusing parameter. Eq (4) in [1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    loss[i..., j] = FL(p) = - (1-p)log(p) Eq (5) in [1].</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">label_probs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">label_probs</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">probs</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
          <span class="n">alpha</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
                           <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">label_ids</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">loss</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">probs</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyFocalLoss"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.SigmoidCrossEntropyFocalLoss">[docs]</a><span class="k">def</span> <span class="nf">SigmoidCrossEntropyFocalLoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sa">u</span><span class="sd">&quot;&quot;&quot;Focal loss for binary (sigmoid) logistic loss.</span>

<span class="sd">  [1] Focal loss https://arxiv.org/abs/1708.02002</span>

<span class="sd">  Args:</span>
<span class="sd">    logits: [..., C]. Logits for the sigmoid logistic regression.</span>
<span class="sd">    labels: [..., C]. 0/1 labels.</span>
<span class="sd">    alpha: The weighting factor alpha. Eq (3) in [1].</span>
<span class="sd">    gamma: Tunable focusing parameter. Eq (4) in [1].</span>

<span class="sd">  Returns:</span>
<span class="sd">    loss[i..., j] = FL(p) = - (1-p)log(p) Eq (5) in [1].</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># [1] Eq (4).</span>
  <span class="c1">#</span>
  <span class="c1"># The numerically-stable way to compute</span>
  <span class="c1">#  log(p) for positives;</span>
  <span class="c1">#  log(1 - p) for negatives.</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># The modulating factor. Note that</span>
    <span class="c1"># (1 - p) = [1 - (x)] = [(-x)], for positives.</span>
    <span class="c1"># p = [(x)], for negatives.</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">labels</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">gamma</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># [1] Eq (3)</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">labels</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">labels</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">loss</span></div>


<span class="n">_RECORD_FORMAT_RE</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;(^[A-Za-z]+):(.*)&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="RecordFormatFromFilePattern"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RecordFormatFromFilePattern">[docs]</a><span class="k">def</span> <span class="nf">RecordFormatFromFilePattern</span><span class="p">(</span><span class="n">file_pattern</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the record format string for a Lingvo file pattern.</span>

<span class="sd">  Lingvo file patterns take the form of:</span>
<span class="sd">    tfrecord:/path/to/bar -&gt; tfrecord is the record_format.</span>

<span class="sd">  This function takes a file pattern and returns a string indicating</span>
<span class="sd">  which format the filepattern implies.</span>

<span class="sd">  Args:</span>
<span class="sd">    file_pattern: String file pattern.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tuple (string, string):</span>

<span class="sd">      - record_format: String record format, e.g., &quot;tfrecord&quot;, etc.</span>
<span class="sd">      - file_pattern: The file pattern without any prefixes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">_RECORD_FORMAT_RE</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># TODO(vrv): Fix all callers so that file_pattern must contain</span>
    <span class="c1"># the record format prefix.</span>
    <span class="k">return</span> <span class="s1">&#39;sstable&#39;</span><span class="p">,</span> <span class="n">file_pattern</span>

  <span class="c1"># regexp ensures that a match implies there are two groups:</span>
  <span class="c1"># the record format and then the file pattern.</span>
  <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span></div>


<div class="viewcode-block" id="ReadFileLines"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadFileLines">[docs]</a><span class="k">def</span> <span class="nf">ReadFileLines</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Read a text file and return the lines.</span>

<span class="sd">  If the file cannot be found at the given path, attempt to load it from the</span>
<span class="sd">  Lingvo package (useful for data dependencies in par files).</span>

<span class="sd">  Args:</span>
<span class="sd">    file_path: path to file, either absolute or relative to the bazel workspace.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of lines from the file.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="n">pkgutil</span><span class="o">.</span><span class="n">get_data</span><span class="p">(</span>
          <span class="s1">&#39;lingvo&#39;</span><span class="p">,</span> <span class="n">file_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;lingvo/&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                      <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">IOError</span><span class="p">:</span>
      <span class="c1"># If pkgutil can&#39;t find the file, continue and let GFile raise the error.</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">lines</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">lines</span></div>


<span class="c1"># Partially borrowed from</span>
<span class="c1"># https://github.com/tensorflow/tensor2tensor/blob/32929305e1a4ec926eff24123758b794df35492b/tensor2tensor/layers/common_layers.py#L349</span>
<div class="viewcode-block" id="CumSum"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CumSum">[docs]</a><span class="k">def</span> <span class="nf">CumSum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A TPU efficient implementation of tf.cumsum().</span>

<span class="sd">  This is equivalent to tf.cumsum and is faster on TPU as of 08/2019 unless</span>
<span class="sd">  the axis dimension is very large. The current Tensorflow implementation is</span>
<span class="sd">  based on scanning and reducing which is not efficient on TPU.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An input Tensor.</span>
<span class="sd">    axis: An int for the axis.</span>
<span class="sd">    exclusive: A bool for performing exclusive cumsum.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Tensor of the same shape as x.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if the input axis is invalid.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">use_tpu</span><span class="p">():</span>
    <span class="c1"># Fallback to tf.cumsum when inputs are not floats or not running on TPU.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">)</span>

  <span class="n">rank</span> <span class="o">=</span> <span class="n">GetRank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="c1"># Needs to know the rank for the final transpose if axis is not the last</span>
  <span class="c1"># dimension. Otherwise, falls back to tf.cumsum.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="n">exclusive</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unexpected axis: </span><span class="si">%d</span><span class="s1"> (rank = </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
    <span class="n">axis</span> <span class="o">+=</span> <span class="n">rank</span>

  <span class="n">length</span> <span class="o">=</span> <span class="n">GetShape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="n">axis</span><span class="p">]</span>
  <span class="n">my_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
  <span class="n">comparator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span> <span class="k">if</span> <span class="n">exclusive</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">less_equal</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">comparator</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">my_range</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">my_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
      <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[[</span><span class="n">axis</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
  <span class="k">if</span> <span class="n">axis</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">!=</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">result</span><span class="p">,</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="ProjectLastDim"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ProjectLastDim">[docs]</a><span class="k">def</span> <span class="nf">ProjectLastDim</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Linear projection on the last dim of the input tensor.</span>

<span class="sd">  This is a TPU efficient implementation to avoid reshaping inputs to Rank-2</span>
<span class="sd">  tensor by using Einsum for the compute.</span>

<span class="sd">  Args:</span>
<span class="sd">    inputs: An input Tensor, the last dimension of which is input_dim.</span>
<span class="sd">    weight: A weight matrix with shape [input_dim, output_dim].</span>
<span class="sd">    input_dim: An integer or a symbolic dim, the last dimension of the inputs.</span>
<span class="sd">    output_dim: An integer or a symbolic dim, the last dimension of the outputs.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An output Tensor of the same rank as inputs, the last dimension is</span>
<span class="sd">    output_dim.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
      <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span> <span class="k">else</span> <span class="n">input_dim</span><span class="p">)</span>
  <span class="n">output_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
      <span class="n">symbolic</span><span class="o">.</span><span class="n">ToStatic</span><span class="p">(</span><span class="n">output_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">symbolic</span><span class="o">.</span><span class="n">IsExpr</span><span class="p">(</span><span class="n">output_dim</span>
                                                      <span class="p">)</span> <span class="k">else</span> <span class="n">output_dim</span><span class="p">)</span>

  <span class="c1"># Assert input_dim and output_dim</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_dim</span><span class="p">)],</span>
                             <span class="n">inputs</span><span class="p">)</span>
  <span class="n">weight</span> <span class="o">=</span> <span class="n">with_dependencies</span><span class="p">([</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">weight</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_dim</span><span class="p">),</span>
      <span class="n">assert_equal</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">weight</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_dim</span><span class="p">)</span>
  <span class="p">],</span> <span class="n">weight</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">use_tpu</span><span class="p">()</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">):</span>
    <span class="c1"># Avoids reshape if feasible and uses Einsum.</span>
    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">chr</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">97</span><span class="p">,</span> <span class="mi">123</span><span class="p">)])</span>  <span class="c1"># abc...xyz</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">rank</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{0}</span><span class="s1">y,yz-&gt;</span><span class="si">{0}</span><span class="s1">z&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">r</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">Matmul</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">ToStaticShape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">])),</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">GetShape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="n">ToStaticShape</span><span class="p">([</span><span class="n">output_dim</span><span class="p">])</span>
        <span class="p">],</span>
                  <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="RemoveAssertContext"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.RemoveAssertContext">[docs]</a><span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Hacks to replace certain unwanted tensorflow ops.&quot;&quot;&quot;</span>
  <span class="c1"># TODO(zhifengc/huangyp): Consider implementing assert_equal</span>
  <span class="c1"># op replacement for lingvo. As assert_equal doesn&#39;t support String on GPUs.</span>
  <span class="c1"># Hack to replace tf.assert_equal</span>
  <span class="c1"># TODO(b/136040013): Remove this after migration to tf.function.</span>
  <span class="k">if</span> <span class="n">remove</span><span class="p">:</span>
    <span class="n">saved_assert_equal</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span>

    <span class="c1"># pylint: disable=unused-argument</span>
    <span class="k">def</span> <span class="nf">NoOP</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>

    <span class="c1"># pylint: enable=unused-argument</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span> <span class="o">=</span> <span class="n">NoOP</span>  <span class="c1"># Make assert_equal a no op.</span>
    <span class="k">yield</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">check_ops</span><span class="o">.</span><span class="n">assert_equal</span> <span class="o">=</span> <span class="n">saved_assert_equal</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">yield</span></div>


<div class="viewcode-block" id="_DefineDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._DefineDefun">[docs]</a><span class="k">def</span> <span class="nf">_DefineDefun</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">bak</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps fwd in a defun with custom gradient bak.</span>

<span class="sd">  Args:</span>
<span class="sd">    fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure.</span>
<span class="sd">    bak: A callable xs, ys, dys: Nested Structure -&gt; dxs: Nested Structure. The</span>
<span class="sd">      custom backprop function for fwd.</span>
<span class="sd">    args: A Nested Structure of tf.Tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A NestedMap w/ fields:</span>
<span class="sd">      defun: A tf.Defun wraps fwd</span>
<span class="sd">      args:  A Nested Structure of tf.DType</span>
<span class="sd">      rets:  A Nested Structure of tf.DType</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">fwd</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

  <span class="c1"># fwd signature (tf.Tensor dtypes).</span>
  <span class="n">get_dtype</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
  <span class="n">sigs</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">Transform</span><span class="p">(</span><span class="n">get_dtype</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>

  <span class="n">get_shape</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">arg_shapes</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">get_shape</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

  <span class="n">compiled</span> <span class="o">=</span> <span class="n">use_xla</span><span class="p">()</span>
  <span class="n">noinline</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">compiled</span>

  <span class="k">def</span> <span class="nf">Backward</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bak</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Note: sigs.rets will be set during the Forward call.</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">rets</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">dys</span> <span class="o">=</span> <span class="n">Pack</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">rets</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">):</span>
      <span class="n">dxs</span> <span class="o">=</span> <span class="n">bak</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">dys</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">dxs</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">args</span><span class="p">),</span> <span class="n">python_grad_func</span><span class="o">=</span><span class="n">Backward</span><span class="p">,</span> <span class="n">noinline</span><span class="o">=</span><span class="n">noinline</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">Forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">arg_shapes</span><span class="p">)):</span>
      <span class="n">arg</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">RemoveAssertContext</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="n">noinline</span><span class="p">):</span>
      <span class="n">rets</span> <span class="o">=</span> <span class="n">fwd</span><span class="p">(</span><span class="n">Pack</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">args</span><span class="p">))</span>
    <span class="n">sigs</span><span class="o">.</span><span class="n">rets</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">get_dtype</span><span class="p">,</span> <span class="n">rets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Flatten</span><span class="p">(</span><span class="n">rets</span><span class="p">)</span>

  <span class="n">sigs</span><span class="o">.</span><span class="n">defun</span> <span class="o">=</span> <span class="n">Forward</span>
  <span class="k">return</span> <span class="n">sigs</span></div>


<div class="viewcode-block" id="CallDefun"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.CallDefun">[docs]</a><span class="k">def</span> <span class="nf">CallDefun</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">bak</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps fwd in a defun with custom gradient bak and calls it with args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fwd: A callable xs: Nested Structure -&gt; ys: Nested Structure.</span>
<span class="sd">    bak: A callable xs, ys, dys: Nested Structure -&gt; dxs: Nested Structure. The</span>
<span class="sd">      custom backprop function for fwd.</span>
<span class="sd">    args: A Nested Structure of tf.Tensor.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A Nested Structure equivalent to what fwd(args) computes.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">sigs</span> <span class="o">=</span> <span class="n">_DefineDefun</span><span class="p">(</span><span class="n">fwd</span><span class="p">,</span> <span class="n">bak</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
  <span class="n">flat_rets</span> <span class="o">=</span> <span class="n">sigs</span><span class="o">.</span><span class="n">defun</span><span class="p">(</span><span class="o">*</span><span class="n">Flatten</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">flat_rets</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
    <span class="n">flat_rets</span> <span class="o">=</span> <span class="p">[</span><span class="n">flat_rets</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">Pack</span><span class="p">(</span><span class="n">sigs</span><span class="o">.</span><span class="n">rets</span><span class="p">,</span> <span class="n">flat_rets</span><span class="p">)</span></div>


<div class="viewcode-block" id="_Itype"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils._Itype">[docs]</a><span class="k">def</span> <span class="nf">_Itype</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Loop iterator data type.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span> <span class="k">if</span> <span class="n">use_xla</span><span class="p">()</span> <span class="k">else</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span></div>


<div class="viewcode-block" id="WhileLoop"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.WhileLoop">[docs]</a><span class="k">def</span> <span class="nf">WhileLoop</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">loop_state</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to construct a while loop.</span>

<span class="sd">  Args:</span>
<span class="sd">    cond: A callable NestedMap -&gt; tf.bool.</span>
<span class="sd">    body: A callable NestedMap -&gt; NestedMap.</span>
<span class="sd">    loop_state: A flattenable (NestedMap, list, tuple, etc.) representing the</span>
<span class="sd">      loop state.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The final loop state in the same structure as loop_state.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span><span class="n">loop_state</span><span class="o">=</span><span class="n">loop_state</span><span class="p">)</span>
  <span class="n">dtypes</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">Transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="n">dtypes</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">LoopCond</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">loop_state</span><span class="p">)</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">Defun</span><span class="p">(</span><span class="o">*</span><span class="n">dtypes</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">LoopBody</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="n">s</span><span class="o">.</span><span class="n">loop_state</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">loop_state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">state</span><span class="o">.</span><span class="n">Pack</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">While</span><span class="p">(</span><span class="n">input_</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">cond</span><span class="o">=</span><span class="n">LoopCond</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">LoopBody</span><span class="p">))</span><span class="o">.</span><span class="n">loop_state</span></div>


<div class="viewcode-block" id="ForLoop"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ForLoop">[docs]</a><span class="k">def</span> <span class="nf">ForLoop</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">loop_state</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper to construct a for loop.</span>

<span class="sd">  Args:</span>
<span class="sd">    body: A callable (tf.int, NestedMap) -&gt; NestedMap.</span>
<span class="sd">    start: Loop variable&#39;s initial value.</span>
<span class="sd">    limit: Loop variable&#39;s limit value.</span>
<span class="sd">    delta: Loop variable&#39;s change per iteration.</span>
<span class="sd">    loop_state: A flattenable (NestedMap, list, tuple, etc.) representing the</span>
<span class="sd">      loop state.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The final loop state in the same structure as loop_state.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">NestedMap</span><span class="p">(</span>
      <span class="nb">iter</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">limit</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">delta</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">_Itype</span><span class="p">()),</span>
      <span class="n">loop_state</span><span class="o">=</span><span class="n">loop_state</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LoopCond</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">limit</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">LoopBody</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">state</span><span class="o">.</span><span class="n">loop_state</span> <span class="o">=</span> <span class="n">body</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">loop_state</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>

  <span class="k">return</span> <span class="n">WhileLoop</span><span class="p">(</span><span class="n">LoopCond</span><span class="p">,</span> <span class="n">LoopBody</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">loop_state</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.TopK">[docs]</a><span class="k">def</span> <span class="nf">TopK</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Equivalent to tf.math.top_k(x_in, k) but more efficient on tpu.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">k</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;This implementation is only efficient for small k.&#39;</span>
  <span class="c1"># TODO(yonghui): Try out an alternative idea where we first reshape x_in as a</span>
  <span class="c1"># 2d tensor, then call tf.math.top_k, and then reshape back.</span>
  <span class="n">x_in_shape</span> <span class="o">=</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">x_rank</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">rank</span>
  <span class="k">assert</span> <span class="n">x_rank</span> <span class="ow">and</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
  <span class="n">last_dim_size</span> <span class="o">=</span> <span class="n">x_in_shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
  <span class="n">min_value</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">x_in</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>

  <span class="n">out_indices</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">out_values</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">unused_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="n">index_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">mask_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">index_i</span><span class="p">,</span> <span class="n">last_dim_size</span><span class="p">)</span>
    <span class="c1"># TODO(yonghui): Would tf.gather be more efficient and numerically stable</span>
    <span class="c1"># here?</span>
    <span class="n">value_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask_i</span> <span class="o">*</span> <span class="n">x_in</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x_in</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mask_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_in</span> <span class="o">+</span> <span class="n">mask_i</span> <span class="o">*</span> <span class="n">min_value</span>
    <span class="n">out_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">index_i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">out_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value_i</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out_values</span><span class="p">,</span> <span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">out_indices</span><span class="p">,</span> <span class="n">x_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReadVariable"><a class="viewcode-back" href="../../../lingvo.core.py_utils.html#lingvo.core.py_utils.ReadVariable">[docs]</a><span class="k">def</span> <span class="nf">ReadVariable</span><span class="p">(</span><span class="n">var_op</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the value of the given variable operation.</span>

<span class="sd">  Args:</span>
<span class="sd">    var_op: The variable&#39;s TF `Operation`. It could be one of VarHandleOp,</span>
<span class="sd">      Variable and VariableV2.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` containing the value of the variable.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">var_op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;VarHandleOp&#39;</span><span class="p">:</span>
    <span class="c1"># Filter out the ReadVariableOps that have control dependencies to avoid</span>
    <span class="c1"># side-effects when the user runs it.</span>
    <span class="n">filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">op</span><span class="p">:</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;ReadVariableOp&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">op</span><span class="o">.</span><span class="n">control_inputs</span>
    <span class="n">var_readers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">filter_fn</span><span class="p">,</span> <span class="n">var_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">consumers</span><span class="p">()))</span>
    <span class="k">assert</span> <span class="n">var_readers</span>
    <span class="k">return</span> <span class="n">var_readers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">assert</span> <span class="n">var_op</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;VariableV2&#39;</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">var_op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>